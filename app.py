import streamlit as st
import google.generativeai as genai
import ssl
import requests
import warnings
from io import StringIO
import yfinance as yf

# -----------------------------------------------------------------------------
# SSL Fix for FinanceDataReader & KRX (User Environment Specific)
# -----------------------------------------------------------------------------
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# Requests Verify Patch
from requests.packages.urllib3.exceptions import InsecureRequestWarning
warnings.simplefilter('ignore', InsecureRequestWarning)

# Monkey-patch requests to disable verification by default
if not hasattr(requests, '_original_get'):
    requests._original_get = requests.get
    requests._original_post = requests.post

    def new_get(*args, **kwargs):
        kwargs['verify'] = False
        return requests._original_get(*args, **kwargs)

    def new_post(*args, **kwargs):
        kwargs['verify'] = False
        return requests._original_post(*args, **kwargs)

    requests.get = new_get
    requests.post = new_post
# -----------------------------------------------------------------------------

import os
import glob
import re
import streamlit.components.v1 as components
import yfinance as yf
import plotly.express as px
import pandas as pd
import pickle # Added for Persistence
import datetime
import json
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error

# -----------------------------------------------------------------------------
# Helper Functions (Moved to Top for Scope Safety)
# -----------------------------------------------------------------------------
import pickle
try:
    import stats_helper # Custom Helper
except ImportError:
    pass 

try:
    from alpha_provider import AlphaFactory
except Exception as e:
    import streamlit as st
    st.error(f"âŒ Critical Error importing alpha_provider: {e}")
    print(f"âŒ Critical Error importing alpha_provider: {e}")
    AlphaFactory = None

MODEL_SAVE_DIR = "saved_models"
if not os.path.exists(MODEL_SAVE_DIR):
    os.makedirs(MODEL_SAVE_DIR)

def save_model_checkpoint(model_name, data):
    try:
        if not os.path.exists(MODEL_SAVE_DIR):
            os.makedirs(MODEL_SAVE_DIR)
        
        filepath = os.path.join(MODEL_SAVE_DIR, f"{model_name}.pkl")
        filepath = filepath.replace(":", "-").replace("|", "_")
        with open(filepath, "wb") as f:
            pickle.dump(data, f)
        return True
    except Exception as e:
        print(f"Error saving model: {e}")
        return False

def load_model_checkpoint(model_name):
    try:
        filepath = os.path.join(MODEL_SAVE_DIR, f"{model_name}.pkl")
        if os.path.exists(filepath):
            with open(filepath, "rb") as f:
                return pickle.load(f)
    except Exception as e:
        print(f"Error loading model: {e}")
    return None

def calculate_feature_set(df, feature_level):
    df = df.copy()
    feature_cols = []

    # 0. Alpha158 (Qlib Exact Match)
    if "Alpha158" in feature_level:
        if AlphaFactory:
            # [Fix] Unpack tuple (df, columns)
            alpha_df, _ = AlphaFactory.get_alpha158(df)
            
            # Merge alphas back to df
            # alpha_df index is MultiIndex (Date, Ticker) or matches df index
            # If df is single ticker, alpha_df might have MultiIndex.
            # Align indices
            if isinstance(alpha_df.index, pd.MultiIndex) and not isinstance(df.index, pd.MultiIndex):
                 # Drop ticker level if single ticker
                 alpha_df = alpha_df.reset_index(level=1, drop=True)
            
            # Join columns
            df = df.join(alpha_df, rsuffix='_alpha') # safety
            feature_cols = alpha_df.columns.tolist()
            return df, feature_cols
        else:
            print("AlphaFactory not found, falling back to Rich")

    # 1. Light (Basic 5)
    if "Light" in feature_level:
        df['MA5'] = df['Close'].rolling(window=5).mean()
        df['MA20'] = df['Close'].rolling(window=20).mean()
        df['Disparity_5'] = df['Close'] / df['MA5']
        df['Disparity_20'] = df['Close'] / df['MA20']
        
        # RSI
        delta = df['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss
        df['RSI'] = 100 - (100 / (1 + rs))
        
        df['Volatility'] = df['Close'].pct_change().rolling(20).std()
        df['Momentum_1M'] = df['Close'].pct_change(20)
        
        feature_cols = ['Disparity_5', 'Disparity_20', 'RSI', 'Volatility', 'Momentum_1M']
    else:
        # Standard(22) or Rich(50+)
        if "Rich" in feature_level:
            windows = [3, 5, 10, 20, 40, 60, 120]
        else:
            windows = [5, 10, 20, 60]

        df['Ret_1d'] = df['Close'].pct_change()
        
        for w in windows:
            col_roc = f'ROC_{w}'
            df[col_roc] = df['Close'].pct_change(w)
            feature_cols.append(col_roc)
            
            col_ma = f'MA_Dist_{w}'
            ma = df['Close'].rolling(window=w).mean()
            df[col_ma] = df['Close'] / ma
            feature_cols.append(col_ma)
            
            col_vol = f'Vol_{w}'
            df[col_vol] = df['Ret_1d'].rolling(window=w).std()
            feature_cols.append(col_vol)
            
            col_vol_ratio = f'Vol_Ratio_{w}'
            vol_ma = df['Volume'].rolling(window=w).mean()
            df[col_vol_ratio] = df['Volume'] / vol_ma
            feature_cols.append(col_vol_ratio)
        
        # RSI
        rsi_windows = [9, 14, 28, 60] if "Rich" in feature_level else [14, 60]
        for rsi_w in rsi_windows:
            delta = df['Close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(rsi_w).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(rsi_w).mean()
            rs = gain / loss
            col_rsi = f'RSI_{rsi_w}'
            df[col_rsi] = 100 - (100 / (1 + rs))
            feature_cols.append(col_rsi)

        # [Rich Only Features]
        if "Rich" in feature_level:
            # Lagged Returns
            for lag in [1, 2, 3, 5]:
                col_lag = f'Ret_Lag_{lag}'
                df[col_lag] = df['Ret_1d'].shift(lag)
                feature_cols.append(col_lag)
            
            # Candle Patterns
            df['Candle_Body'] = (df['Close'] - df['Open']).abs()
            df['Candle_Len'] = (df['High'] - df['Low'])
            df['Body_Ratio'] = df['Candle_Body'] / df['Candle_Len'].replace(0, 1)
            feature_cols.append('Body_Ratio')
            
            df['Shadow_Upper'] = (df['High'] - df[['Open', 'Close']].max(axis=1)) / df['Candle_Len'].replace(0, 1)
            df['Shadow_Lower'] = (df[['Open', 'Close']].min(axis=1) - df['Low']) / df['Candle_Len'].replace(0, 1)
            feature_cols.append('Shadow_Upper')
            feature_cols.append('Shadow_Lower')
            
            # Day of Week
            df['DayOfWeek'] = df.index.dayofweek
            feature_cols.append('DayOfWeek')
            
    feature_cols = list(set(feature_cols)) # Ensure unique
    return df, feature_cols

# -----------------------------------------------------------------------------
# Portfolio & Universe Helpers (Moved to Top for Scope Safety)
# -----------------------------------------------------------------------------
PORT_SAVE_DIR = "saved_portfolios"
if not os.path.exists(PORT_SAVE_DIR):
    os.makedirs(PORT_SAVE_DIR)

PORTFOLIO_HISTORY_FILE = os.path.join(PORT_SAVE_DIR, "ai_portfolio_history.json")

def load_portfolio_history():
    if os.path.exists(PORTFOLIO_HISTORY_FILE):
        try:
            with open(PORTFOLIO_HISTORY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_portfolio_history(history_data):
    try:
        with open(PORTFOLIO_HISTORY_FILE, "w", encoding="utf-8") as f:
            json.dump(history_data, f, ensure_ascii=False, indent=4)
    except Exception as e:
        print(f"Error saving portfolio history: {e}")

# NASDAQ 100 Full List (Static)
NASDAQ_100_FULL = [
    "AAPL", "MSFT", "NVDA", "GOOGL", "AMZN", "META", "TSLA", "AVGO", "ADBE", "COST",
    "PEP", "CSCO", "NFLX", "AMD", "TMUS", "INTC", "TXN", "QCOM", "AMGN", "HON",
    "AMAT", "INTU", "SBUX", "ADP", "BKNG", "GILD", "ISRG", "MDLZ", "REGN", "VRTX",
    "LRCX", "ADI", "PANW", "MU", "SNPS", "CDNS", "CHTR", "KLAC", "CSX", "MAR",
    "CRWD", "MELI", "NXPI", "ORLY", "CTAS", "MNST", "ROP", "LULU", "ODFL", "PCAR",
    "PAYX", "FTNT", "KDP", "EXC", "XEL", "IDXX", "BIIB", "AEP", "MCHP", "ALGN",
    "DLTR", "EA", "AZN", "WBD", "FAST", "CTSH", "BKR", "GFS", "VRSK", "KHC",
    "GEHC", "TEAM", "SGEN", "ZS", "DDOG", "FANG", "ON", "ANSS", "CDW", "TTD",
    "WBA", "ILMN", "SIRI", "ZM", "ENPH", "JD", "PDD", "BIDU", "NTES", "CEG",
    "FISV", "ATVI", "MRVL", "MRNA", "DXCM", "LCID", "RIVN", "WDAY", "EBAY", "SPLK"
]
import lightgbm as lgb
import numpy as np
import numpy as np
import scipy.optimize as sco
from pykrx import stock
import time
from datetime import datetime, timedelta


# 1. API í‚¤ ì„¤ì • (Google AI Studioì—ì„œ ë°œê¸‰ë°›ì€ í‚¤ ì…ë ¥)
# ì•ˆì „í•œ ë°©ì‹ (Streamlit Secrets ì‚¬ìš©)
# -----------------------------------------------------------------------------
# 1. API í‚¤ ì„¤ì • (Rotation Logic)
# -----------------------------------------------------------------------------
# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  API í‚¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
api_keys = []

# 1. Streamlit Secrets ìš°ì„  í™•ì¸
if "GOOGLE_API_KEY" in st.secrets:
    api_keys.append(st.secrets["GOOGLE_API_KEY"])
    # ì¶”ê°€ í‚¤ í™•ì¸ (GOOGLE_API_KEY_2, _3 ...)
    i = 2
    while f"GOOGLE_API_KEY_{i}" in st.secrets:
        api_keys.append(st.secrets[f"GOOGLE_API_KEY_{i}"])
        i += 1
else:
    # 2. í™˜ê²½ ë³€ìˆ˜ í™•ì¸ (ë¡œì»¬ í…ŒìŠ¤íŠ¸)
    key = os.getenv("GOOGLE_API_KEY")
    if key:
        api_keys.append(key)
        # ì¶”ê°€ í‚¤ í™•ì¸
        i = 2
        while os.getenv(f"GOOGLE_API_KEY_{i}"):
            api_keys.append(os.getenv(f"GOOGLE_API_KEY_{i}"))
            i += 1

if not api_keys:
    st.error("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .streamlit/secrets.toml ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# ê¸°ë³¸ í‚¤ë¡œ ì´ˆê¸° ì„¤ì •
genai.configure(api_key=api_keys[0])

def generate_content_with_rotation(prompt, model_name="gemini-3-flash-preview"):
    """
    API í‚¤ë¥¼ ìˆœí™˜í•˜ë©° ì»¨í…ì¸  ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤.
    Rate Limit ë°œìƒ ì‹œ ë‹¤ìŒ í‚¤ë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤.
    """
    last_error = None
    
    for i, key in enumerate(api_keys):
        try:
            # í˜„ì¬ í‚¤ë¡œ ì„¤ì • ë° ìƒì„± ì‹œë„
            genai.configure(api_key=key)
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            return response.text
            
        except Exception as e:
            last_error = e
            # Quota ê´€ë ¨ ì—ëŸ¬ì¸ ê²½ìš° ë‹¤ìŒ í‚¤ ì‹œë„
            # (429 Resource exhausted, Quota exceeded ë“±)
            error_str = str(e)
            if "429" in error_str or "Quota" in error_str or "Resource exhausted" in error_str:
                # ë§ˆì§€ë§‰ í‚¤ê°€ ì•„ë‹ˆë©´ ë‹¤ìŒ í‚¤ë¡œ ê³„ì†
                if i < len(api_keys) - 1:
                    continue
            
            # ê·¸ ì™¸ ì—ëŸ¬ê±°ë‚˜ ë§ˆì§€ë§‰ í‚¤ë©´ ë£¨í”„ ì¢…ë£Œ (After loop raises)
            break
            
    # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë°œìƒ
    raise last_error

# -----------------------------------------------------------------------------

# 2. FAQ ë°ì´í„° ì •ì˜ (ì—¬ê¸°ì— ì¤€ë¹„í•˜ì‹  FAQ ë‚´ìš©ì„ ììœ ë¡­ê²Œ ë„£ìœ¼ì„¸ìš”)
faq_data = """
[ë¡œë³´ì–´ë“œë°”ì´ì € ì„œë¹„ìŠ¤ ìƒì„¸ ë§¤ë‰´ì–¼]

### 1. ì„œë¹„ìŠ¤ ê¸°ëŠ¥ ë° ê¸°ë³¸ ì›ì¹™
- **ê°€ì… ë° ì„¤ê³„**: ê°€ì…ê³¼ ë™ì‹œì— ë§ì¶¤ì„¤ê³„ê°€ ì§„í–‰ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë©°, ê³ ê°ì´ ê°€ì… í›„ ì§ì ‘ ë§ì¶¤ì„¤ê³„ë¥¼ ì§„í–‰í•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” ê³ ê°ì—ê²Œ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ 'ì¶”ì²œ'ë“œë¦¬ëŠ” ì„œë¹„ìŠ¤ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
- **íˆ¬ìì„±í–¥**: íˆ¬ììì„±í–¥ê³¼ ìƒê´€ì—†ì´ ê°€ì…ì€ ê°€ëŠ¥í•˜ë©°, ìµœì¢…ì—ëŠ” ë³¸ì¸ íˆ¬ìì„±í–¥ì— ë”°ë¥¸ í¬íŠ¸í´ë¦¬ì˜¤ ìœ í˜•ì´ ì„ íƒë˜ì§€ë§Œ íƒ€ ìœ í˜•ë„ ì„ íƒ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë‹¨, ë§ì¶¤ì„¤ê³„ ê³¼ì •ì—ì„œ íˆ¬ìë¶€ì í•©ì„± ì•ˆë‚´, ì•½ê´€ë™ì˜ ë“± í•„ìˆ˜ ê³ ì§€ì‚¬í•­ í”„ë¡œì„¸ìŠ¤ê°€ ì¶”ê°€ ë°œìƒí•©ë‹ˆë‹¤.
- **í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘ ìˆ˜ì •**: íˆ¬ììê°€ ì„ì˜ë¡œ ìì‚°êµ° ë¹„ì¤‘ì„ ìˆ˜ì •í•˜ê±°ë‚˜ ì¼ë¶€ í€ë“œë§Œ êµì²´í•˜ê²Œ ì„¤ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‡´ì§ì—°ê¸ˆì˜ ìœ„í—˜ìì‚°ë¹„ìœ¨ ì¤€ìˆ˜ë¥¼ ìœ„í•´ ë¡œë³´ì–´ë“œë°”ì´ì €ê°€ ë§¤ë§¤ ì‹œ ìë™ìœ¼ë¡œ ë¹„ì¤‘ ì¤€ìˆ˜ë¥¼ ìœ„í•œ ë§¤ë§¤ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
- **í€ë“œ êµì²´ ë²”ìœ„**: ì›ì¹™ì ìœ¼ë¡œ ê³ ê°ì´ ê°€ì§€ê³  ê³„ì‹  ê³µëª¨í€ë“œ ì „ì²´ê°€ êµì²´ ëŒ€ìƒì…ë‹ˆë‹¤. ë‹¨, ë¡œë³´ì–´ë“œë°”ì´ì €ê°€ íŒë‹¨í•˜ê¸°ì— 1ìœ„ ìƒí’ˆê³¼ ì„±ëŠ¥ì´ ìœ ì‚¬í•œ í€ë“œëŠ” ì¼ë¶€ë§Œ ë§¤ë§¤ë  ìˆ˜ ìˆìœ¼ë©°, ê±°ë˜ ë²”ì£¼ì—ì„œ ì œì™¸ë˜ëŠ” í•­ëª©ì€ ë§¤ë§¤ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.

### 2. ê°€ì… ë¶ˆê°€ ìš”ê±´ ìƒì„¸ (ê° ê³„ì¢Œë³„)
- **í‡´ì§ì—°ê¸ˆ**: MPêµ¬ë… ì„œë¹„ìŠ¤ ì´ìš© ê³„ì¢Œ ë“±.
- **ê°œì¸ì—°ê¸ˆ**: ì—°ê¸ˆê°œì‹œ ì •ê¸°ì§€ê¸‰ ê³„ì¢Œ(ì„ì˜ì‹ì€ ê°€ëŠ¥), ëŒ€ì¶œ ì•½ì •ê³„ì¢Œ, ì—°ê¸ˆì €ì¶•ê³„ì¢Œ ì •ê¸°ë§¤ë„ ì•½ì •ê³„ì¢Œ, ìë™ëŒ€ì²´ì…ê¸ˆë§¤ìˆ˜ ì•½ì •ê³„ì¢Œ, CMSìë™ëŒ€ì²´ì…ê¸ˆë§¤ìˆ˜ ì•½ì •ê³„ì¢Œ, í€ë“œ ì •ê¸°ìë™ë§¤ìˆ˜ ì•½ì •ê³„ì¢Œ, ì´ì „ìš© ê³„ì¢Œ, ì´ê´€ì‹ ì²­ì¤‘ ê³„ì¢Œ, ì‚¬ê³ ê³„ì¢Œ(ë§¤ë§¤ì œí•œ) ë° ì¥ê¸°ë¯¸ì‚¬ìš© ê³„ì¢Œ. íƒ€ ì„œë¹„ìŠ¤ ì´ìš© ì¤‘ì¸ ê²½ìš°(ê°œì¸ì—°ê¸ˆ ë©, ê°œì¸ì—°ê¸ˆ ìë¬¸, ì ë¦½ì‹ ìë™ë§¤ìˆ˜ ì„œë¹„ìŠ¤-ì—°ê¸ˆ ëª¨ìœ¼ê¸°) ê°€ì… ë¶ˆê°€.
- **ISA**: ê³„ì¢Œí•´ì§€ ì‹ ì²­ì¤‘ ë° ì´í›„ë‹¨ê³„ ê³„ì¢Œ, ìë™ëŒ€ì²´ì…ê¸ˆë§¤ìˆ˜ ì•½ì •ê³„ì¢Œ, CMSìë™ëŒ€ì²´ì…ê¸ˆë§¤ìˆ˜ ì•½ì •ê³„ì¢Œ, í€ë“œ ì •ê¸°ìë™ë§¤ìˆ˜ ì•½ì •ê³„ì¢Œ, ì‚¬ê³ ê³„ì¢Œ(ë§¤ë§¤ì œí•œ), ì ë¦½ì‹ ìë™ë§¤ìˆ˜ ì„œë¹„ìŠ¤, ì´ê´€ì‹ ì²­ì ‘ìˆ˜/ì´ê´€í•´ì§€ì‹ ì²­ ê³„ì¢Œ, ë§Œê¸°ì´ˆê³¼ ê³„ì¢Œ.
- **ì¼ë°˜ê³„ì¢Œ**: ê³„ì¢Œí•´ì§€ ì‹ ì²­ì¤‘ ë° ì´í›„ë‹¨ê³„ ê³„ì¢Œ, ìë™ëŒ€ì²´ì…ê¸ˆë§¤ìˆ˜ ì•½ì •ê³„ì¢Œ, CMSìë™ëŒ€ì²´ì…ê¸ˆë§¤ìˆ˜ ì•½ì •ê³„ì¢Œ, ì‚¬ê³ ê³„ì¢Œ(ë§¤ë§¤ì œí•œ), ì ë¦½ì‹ ìë™ë§¤ìˆ˜ ì„œë¹„ìŠ¤, ì´ê´€ì‹ ì²­ì ‘ìˆ˜/ì´ê´€í•´ì§€ì‹ ì²­ ê³„ì¢Œ, ë§Œê¸°ì´ˆê³¼ ê³„ì¢Œ, ì‹ ìš©/ëŒ€ì¶œ/ëŒ€ì—¬/ì œíœ´ ì•½ì •ê³„ì¢Œ, ê³µëª¨ë¶€ë™ì‚°ë¶„ë¦¬ê³¼ì„¸ ì•½ì •, ë¶„ë¦¬ê³¼ì„¸í•˜ì´ì¼ë“œ ì•½ì •, ë¶„ë¦¬ê³¼ì„¸ê³ ìœ„í—˜ê³ ìˆ˜ìµí€ë“œ ì•½ì •, ì›”ì§€ê¸‰ ì•½ì •, ê³„ì¢Œì¦ê±°ê¸ˆë¥  100% ì™¸, í•´ì™¸ì£¼ì‹ ê³„ì¢Œì¦ê±°ê¸ˆë¥  100% ì™¸, ê³„ì¢Œìœ„íƒì¦ê±°ê¸ˆ ë¯¸ì§•ìˆ˜, ë§¤ë§¤í—ˆìš© ìœ ê°€ì¦ê¶Œ 'í€ë“œ' ë¯¸ë“±ë¡ ê³„ì¢Œ. ë©ê³„ì•½ ì•½ì •ê³„ì¢Œ, ìë¬¸ì‚¬ ì¼ì„/ìë¬¸ ê³„ì¢Œ ì´ìš© ì‹œ ë¶ˆê°€.
- **ë¹„ê³¼ì„¸ì¢…í•©ì €ì¶•**: ìœ„ ì¼ë°˜ê³„ì¢Œ ìš”ê±´ê³¼ ë™ì¼í•¨.

### 3. ë§ì¶¤ì„¤ê³„ ì´ìš© ì œí•œ ì—¬ë¶€ ë° ì œì™¸ ìƒí’ˆ (MAPIS ê¸°ì¤€)
- **ì •ìƒ ìƒíƒœ ê¸°ì¤€ (MAPIS 7895, 8525)**: 
  * íˆ¬ììì„±í–¥: ì„±ì¥í˜•, ì„±ì¥ì¶”êµ¬í˜• ë“± (ì•ˆì •ì¶”êµ¬í˜• ë“± ë¶€ì í•© ì‹œ 'í•´ë‹¹'ìœ¼ë¡œ í‘œì‹œë¨)
  * íˆ¬ìê¶Œìœ : 'í¬ë§' ìƒíƒœì—¬ì•¼ í•¨
  * ìš´ìš©ê°€ëŠ¥ê¸ˆì•¡: 10,000ì› ì´ìƒ
  * ìœ„í—˜ìì‚°ë¹„ìœ¨: í‡´ì§ì—°ê¸ˆì˜ ê²½ìš° 70% ì´í•˜
  * ë³´ìœ ìƒí’ˆê°¯ìˆ˜: í‡´ì§ì—°ê¸ˆ 20ê°œ ë¯¸ë§Œ, ê°œì¸ì—°ê¸ˆ/ISA ë“± 50ê°œ ì´í•˜
- **ìš´ìš© ë° í‰ê°€ ì œì™¸ í€ë“œ ë¦¬ìŠ¤íŠ¸**: ì•„ë˜ í€ë“œëŠ” í€ë“œí‰ê°€ê¸ˆì•¡ ë° ìš´ìš©ê°€ëŠ¥ê¸ˆì•¡ ì§‘ê³„ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.
  1) ê±°ë˜ë¶ˆê°€í€ë“œ (ì˜ˆ: ëŸ¬ì‹œì•„ í€ë“œ ë“±)
  2) í™˜ë§¤ìˆ˜ìˆ˜ë£Œ ë°œìƒ í€ë“œ
  3) ì‚¬ëª¨í€ë“œ
  4) ì˜¤í”„ë¼ì¸ì „ìš©í€ë“œ
  5) í™˜ë§¤ê¸ˆì§€í€ë“œ
  6) ì„±ê³¼ë³´ìˆ˜ í€ë“œ
  7) ì½”ìŠ¤ë‹¥ë²¤ì²˜ í€ë“œ
  8) ìˆ™ë ¤ëŒ€ìƒ í€ë“œ

### 4. 2026/1/1 ìµœì‹  ì œí•œ ë° ì—…ë°ì´íŠ¸ ì‚¬í•­
- **í˜„ì¬ìê¸ˆíˆ¬ìì„±í–¥ ì œí•œ**: Q1 'ë‹¨ê¸° ìƒê³„ ìê¸ˆ' í˜¹ì€ Q2 'ì›ê¸ˆ ë³´ì¡´ ì¶”êµ¬' ë‹µë³€ ì¤‘ í•˜ë‚˜ë¼ë„ ì²´í¬ëœ ê²½ìš° ì´ìš© ë¶ˆê°€. (MAPIS 3250ì—ì„œ í™•ì¸ ë° ì¬ì§„ë‹¨ í•„ìš”)
- **ISA í•´ì§€ ê´€ë ¨**: ISA ë¶€ì í•© ìš”ê±´ ë°œìƒ í˜¹ì€ ê³„ì¢Œ ì´ì „ ì‹ ì²­ ì‹œ, í•´ë‹¹ ê³„ì¢Œì˜ ë¡œë³´ì–´ë“œë°”ì´ì € ê°€ì…ì„ ì‚¬ì „ì ìœ¼ë¡œ í•´ì§€í•´ì•¼ ì—…ë¬´ ì§„í–‰ì´ ê°€ëŠ¥í•¨.
- **íˆ¬ìì„¤ëª…ì„œ ë°œì†¡ (25/10/24 ì¶”ê°€)**:
  * í‡´ì§ì—°ê¸ˆ: ë§ì¶¤ì„¤ê³„ ì™„ë£Œ ì§í›„ 1íšŒë§Œ ì•Œë¦¼í†¡ ë°œì†¡.
  * ê°œì¸ì—°ê¸ˆ/ISA/ì¼ë°˜: ë§¤ìˆ˜ë˜ëŠ” í€ë“œë§ˆë‹¤ ë§¤ìˆ˜ ì‹œì ì— ê°œë³„ íˆ¬ìì„¤ëª…ì„œ ë°œì†¡.

### 5. ë§¤ë§¤, ìˆ˜ìµë¥  ë° ì•Œë¦¼ ê·œì¹™
- **ë§¤ë§¤ ë¶ˆê°€ ì‹œê°„**: 23ì‹œ 55ë¶„ ~ 24ì‹œ 05ë¶„ (ì£¼ë¬¸ ì œì¶œ ì‹œ ì‹¤íŒ¨ ë° ì „ì²´ ì·¨ì†Œ ì²˜ë¦¬).
- **ë¦¬ë°¸ëŸ°ì‹± ì•Œë¦¼**: ìˆ˜ì‹œ(ì§ì „ ìŠ¹ì¸ 5ì˜ì—…ì¼ ê²½ê³¼ ë° ë¹„ì¤‘ ì°¨ì´ ë°œìƒ ì‹œ, ì•½ 14ì¼ ì£¼ê¸° ê²€ì¶œ), ì •ê¸°(ìµœì¢… ìŠ¹ì¸ í›„ 40ì˜ì—…ì¼ ê²½ê³¼ ì‹œ).
- **ìˆ˜ìµë¥  í™•ì¸ ë¶ˆê°€ ì‚¬ìœ **: ê³„ì¢Œ ë‚´ ë¡œë³´ ë¯¸ìš´ìš© ìƒí’ˆ(ì˜ˆê¸ˆ, ELB, ETF ë“±) ì¡´ì¬ë¡œ ì¸í•œ í˜¼ë™ ë°©ì§€ ë° ê³ ê° ì˜ì‚¬(ìŠ¹ì¸/ê±°ì ˆ) ê°œì…ì— ë”°ë¥¸ ì„±ê³¼ ì°¨ì´ ë•Œë¬¸.
- **ì„±ê³¼ í™•ì¸ ê²½ë¡œ**: [MY ë¡œë³´ì–´ë“œë°”ì´ì € > ê³„ì¢Œí˜„í™© > ë³´ìœ í€ë“œ] ë˜ëŠ” [MY í€ë“œ] í™”ë©´.

### 6. ì£¼ìš” ì—ëŸ¬ ì‚¬ë¡€ (Error Case)
1) **ì†Œìˆ˜ì  ì²˜ë¦¬**: í‡´ì§ì—°ê¸ˆì—ì„œ ì•„ì£¼ ì ì€ ê¸ˆì•¡ íˆ¬ì ì‹œ ë¹„ì¤‘ ë‹¨ìœ„ê°€ ì •ìˆ˜ì´ê¸° ë•Œë¬¸ì— 'ë§¤ë„ ìƒí’ˆ ì—†ìŒ' ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥.
2) **ìœ„í—˜ìì‚° ë¹„ì¤‘ ì‹œì°¨**: ë‹¹ì¼ ë§¤ë§¤ë¡œ ìœ„í—˜ìì‚°ë¹„ìœ¨ 70% ì´ˆê³¼ ìƒíƒœì—ì„œ ì„¤ê³„ ì‹œ ì¥ì•  ë°œìƒ. ê²°ì œ ì™„ë£Œ ì‹œê¹Œì§€ ëŒ€ê¸° í•„ìš”.
3) **ì¤‘ë³µ í”„ë¡œì„¸ìŠ¤**: 'í¬íŠ¸í´ë¦¬ì˜¤ ë³€ê²½ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤' íŒì—… ì‹œ ê¸°ì¡´ ë§¤ë§¤ ìŠ¤ì¼€ì¤„ ì·¨ì†Œ í›„ ì¬ì§„í–‰.
4) **ë¯¸êµ­ êµ­ì ì**: ë§¤ë§¤ ë¶ˆê°€ í€ë“œ í¬í•¨ ì‹œ ë§ì¶¤ì„¤ê³„ ë‹¨ê³„ì—ì„œ ì—ëŸ¬.
5) **í‡´ì‚¬ í›„ DC ê³„ì¢Œ**: ê°€ì…ì ë²ˆí˜¸ê°€ ë‚¨ì•„ í™”ë©´ ì§„ì…ì€ ê°€ëŠ¥í•˜ë‚˜ ì„¤ê³„ ì‹œ ì—ëŸ¬ ë°œìƒ (ì°¨ì„¸ëŒ€ ì´í›„ ìˆ˜ì • ì˜ˆì •).
"""

# 3. ëª¨ë¸ ì„¤ì • (Gemini 3 Flash ì‚¬ìš©)
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— FAQ ë°ì´í„°ë¥¼ ì£¼ì…í•©ë‹ˆë‹¤.
system_prompt = f"""
ë‹µë³€ì˜ 1ìˆœìœ„ ê·¼ê±°ëŠ” ì œê³µëœ **[FAQ ë°ì´í„°]**ì…ë‹ˆë‹¤.
ë§Œì•½ FAQì— ì—†ëŠ” ë‚´ìš© ì¤‘ ì¼ë°˜ì ì¸ ê¸ˆìœµ ì§€ì‹ì€ ë‹¹ì‹ ì˜ ê¸°ë³¸ ì§€ì‹ì„ í™œìš©í•´ ì„¤ëª…í•˜ë˜, ë¯¸ë˜ì—ì…‹ì˜ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ì •ì±…ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
ë¯¼ê°í•œ íˆ¬ì ê¶Œìœ  ì§ˆë¬¸ì—ëŠ” FAQì˜ ê³µì‹ ì…ì¥ì„ ì „ë‹¬í•˜ì„¸ìš”.
ì¼ë°˜ì ì¸ ì§€ì‹ì´ ì•„ë‹ˆê³ , [FAQ ë°ì´í„°]ì— ì—†ëŠ” ë‚´ìš©ì´ë¼ë©´ "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ë‚´ìš©ì€ ê³ ê°ì„¼í„°(1588-XXXX)ë¡œ ë¬¸ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”."
[FAQ ë°ì´í„°]
{faq_data}
"""

model = genai.GenerativeModel(
    model_name="gemini-3-flash-preview",
    system_instruction=system_prompt
)

# 4. ì›¹ í™”ë©´ UI êµ¬ì„± (Streamlit)
st.set_page_config(page_title="ë¡œë³´ì–´ë“œë°”ì´ì € ì±—ë´‡", page_icon="ğŸ¤–", layout="wide")

# OP.GG ìŠ¤íƒ€ì¼ ì»¤ìŠ¤í…€ CSS ì ìš© (Light Theme)
st.markdown(
    """
    <style>
        /* ê¸°ë³¸ í°íŠ¸ ì„¤ì • */
        @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&display=swap');
        
        html, body, [class*="css"] {
            font-family: 'Noto Sans KR', sans-serif;
        }

        /* ë©”ì¸ ë°°ê²½ìƒ‰ - OP.GGì˜ ë°ì€ íšŒìƒ‰/ë¸”ë£¨ í†¤ */
        .stApp {
            background-color: #ecf2f5;
            color: #23292f; /* ì§™ì€ íšŒìƒ‰ í…ìŠ¤íŠ¸ */
        }

        /* ì‚¬ì´ë“œë°” ë°°ê²½ìƒ‰ - OP.GGì˜ ì§™ì€ ë„¤ì´ë¹„ (í—¤ë” ëŠë‚Œ) */
        [data-testid="stSidebar"] {
            background-color: #1c2836;
        }
        
        /* ì‚¬ì´ë“œë°” ë‚´ í…ìŠ¤íŠ¸ ìƒ‰ìƒ ì¡°ì • (ë” êµ¬ì²´ì ìœ¼ë¡œ) */
        [data-testid="stSidebar"] h1, [data-testid="stSidebar"] h2, [data-testid="stSidebar"] h3, [data-testid="stSidebar"] label, [data-testid="stSidebar"] p {
            color: #ffffff !important;
        }

        /* ë¼ë””ì˜¤ ë²„íŠ¼ ìŠ¤íƒ€ì¼ ì»¤ìŠ¤í…€ (ì‚¬ì´ë“œë°” ë©”ë‰´) */
        [data-testid="stSidebar"] [data-testid="stRadio"] label {
            background-color: transparent;
            color: #b0b8c1 !important; /* ê¸°ë³¸: íšŒìƒ‰ */
            padding: 10px;
            border-radius: 4px;
            transition: all 0.2s;
            margin-bottom: 2px;
            cursor: pointer;
        }
        
        /* ë¼ë””ì˜¤ ë²„íŠ¼ ì„ íƒëœ í•­ëª© */
        [data-testid="stSidebar"] [data-testid="stRadio"] label[data-checked="true"] {
             background-color: #5383e8 !important; /* ì„ íƒì‹œ ë¸”ë£¨ ë°°ê²½ */
             color: #ffffff !important; /* ì„ íƒì‹œ í°ê¸€ì”¨ */
             font-weight: bold;
        }
        
        /* ë¼ë””ì˜¤ ë²„íŠ¼ í˜¸ë²„ íš¨ê³¼ */
        [data-testid="stSidebar"] [data-testid="stRadio"] label:hover {
             background-color: #24354a; /* í˜¸ë²„ì‹œ ì•½ê°„ ë°ì€ ë„¤ì´ë¹„ */
             color: #ffffff !important;
        }

        /* í—¤ë” ë°°ê²½ìƒ‰ */
        [data-testid="stHeader"] {
            background-color: rgba(0,0,0,0);
        }

        /* ì œëª© ìƒ‰ìƒ (OP.GG ë¸Œëœë“œ ë¸”ë£¨ í¬ì¸íŠ¸) */
        h1 {
            color: #5383e8 !important;
            font-weight: 700;
        }
        h2, h3 {
            color: #23292f !important;
        }

        /* ì±„íŒ… ì…ë ¥ì°½ ìŠ¤íƒ€ì¼ (í™”ì´íŠ¸ ë°•ìŠ¤) */
        div[data-testid="stChatInput"] > div {
            background-color: #ffffff !important;
            border: 1px solid #dce2f0 !important;
            border-radius: 4px; /* ì‚´ì§ ëœ ë‘¥ê¸€ê²Œ */
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }

        /* ì…ë ¥ì°½ í…ìŠ¤íŠ¸ ì˜ì—­ */
        div[data-testid="stChatInput"] textarea {
            background-color: transparent !important;
            color: #23292f !important; /* ì–´ë‘ìš´ ê¸€ì”¨ */
        }
        
        /* í”Œë ˆì´ìŠ¤í™€ë” í…ìŠ¤íŠ¸ */
        div[data-testid="stChatInput"] textarea::placeholder {
            color: #9aa4af !important;
        }

        /* í¬ì»¤ìŠ¤ íš¨ê³¼ (ë¸Œëœë“œ ë¸”ë£¨) */
        div[data-testid="stChatInput"] > div:focus-within {
            border-color: #5383e8 !important;
            box-shadow: 0 0 0 1px #5383e8 !important;
        }

        /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ (ë¸Œëœë“œ ë¸”ë£¨) */
        .stButton button {
            background-color: #5383e8;
            color: white;
            border: none;
            border-radius: 4px;
            font-weight: bold;
        }
        .stButton button:hover {
            background-color: #426cb7;
            color: white;
        }

        /* ë©”ì‹œì§€ ë°•ìŠ¤ ìŠ¤íƒ€ì¼ (ì±„íŒ… í’ì„  ëŠë‚Œ) */
        .stChatMessage {
            background-color: transparent;
        }
        
        /* ì‚¬ìš©ì/AI ë©”ì‹œì§€ êµ¬ë¶„ê° (ì„ íƒ ì‚¬í•­) */
        [data-testid="chatAvatarIcon-user"] {
            background-color: #5383e8;
        }
        [data-testid="chatAvatarIcon-assistant"] {
            background-color: #ffb900; /* AIëŠ” ë…¸ë€ìƒ‰ í¬ì¸íŠ¸ */
        }
    </style>
    """,
    unsafe_allow_html=True
)

# ì‚¬ì´ë“œë°” ë„¤ë¹„ê²Œì´ì…˜
with st.sidebar:
    st.title("ğŸš€ Antigravity")
    
    selection = st.radio(
        "ë©”ë‰´ ì„ íƒ", 
        ["ğŸ  í™ˆ (Dashboard)", "ğŸ“„ Macro Talking Point", "ğŸ¤– ë¡œë³´ ì–´ë“œë°”ì´ì € (Demo)", "ğŸ” ETF êµ¬ì„± ì¢…ëª© ê²€ìƒ‰", "ğŸ¤– AI ëª¨ë¸ í…ŒìŠ¤íŒ…", "ğŸ§ª Qlib ì‹¤í—˜ì‹¤ (Pro)", "ğŸ” ê¸°ìˆ ì  íŒ¨í„´ ìŠ¤ìºë„ˆ", "âš–ï¸ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”", "ğŸ¤– ì±—ë´‡"]
    )

import requests

# -----------------------------------------------------------------------------
# Helper Functions for Ticker Fetching
# -----------------------------------------------------------------------------
@st.cache_data
def get_sp500_tickers():
    """Wikipediaì—ì„œ S&P 500 ì¢…ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers)
        tables = pd.read_html(response.text)
        df = tables[0]
        tickers = df['Symbol'].tolist()
        return [t.replace('.', '-') for t in tickers] # BRK.B -> BRK-B ë³€í™˜
    except Exception as e:
        st.error(f"S&P 500 ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
        return []

@st.cache_data
def get_nasdaq100_tickers():
    """Wikipediaì—ì„œ NASDAQ 100 ì¢…ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        url = 'https://en.wikipedia.org/wiki/Nasdaq-100'
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers)
        tables = pd.read_html(response.text)
        # í…Œì´ë¸” ì¸ë±ìŠ¤ê°€ ë°”ë€” ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—´ ì´ë¦„ìœ¼ë¡œ í™•ì¸
        for table in tables:
            if 'Ticker' in table.columns:
                return [t.replace('.', '-') for t in table['Ticker'].tolist()]
            elif 'Symbol' in table.columns:
                return [t.replace('.', '-') for t in table['Symbol'].tolist()]
        return []
    except Exception as e:
        st.error(f"NASDAQ 100 ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
        return []



if selection == "ğŸ  í™ˆ (Dashboard)":
    st.title("ğŸ  Antigravity Dashboard")
    st.markdown("### ğŸš€ AIê°€ ë¶„ì„í•˜ëŠ” ì‹¤ì „ íŠ¸ë ˆì´ë”©/íˆ¬ì í”Œë«í¼")
    
    # Simple Dashboard Widgets
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("S&P 500 (SPY)", "Live", "checking...")
    with col2:
        st.metric("NASDAQ 100 (QQQ)", "Live", "checking...")
    with col3:
        st.metric("KOSPI 200", "Live", "checking...")
        
    st.divider()
    
    st.info("ğŸ‘ˆ **ì‚¬ì´ë“œë°”ì—ì„œ ì›í•˜ëŠ” ë¶„ì„ ë„êµ¬ë¥¼ ì„ íƒí•˜ì„¸ìš”.**")
    
    c1, c2 = st.columns(2)
    with c1:
        st.markdown("#### ğŸ¤– ì£¼ìš” ê¸°ëŠ¥")
        st.markdown("""
        - **ğŸ¤– ë¡œë³´ ì–´ë“œë°”ì´ì €**: ë‚´ ì„±í–¥ì— ë§ëŠ” ETF í¬íŠ¸í´ë¦¬ì˜¤
        - **ğŸ” ETF ì—­ê²€ìƒ‰**: ë‚´ê°€ ì›í•˜ëŠ” ì¢…ëª©ì„ ë‹´ì€ ETF ì°¾ê¸°
        - **ğŸ§ª Qlib ì‹¤í—˜ì‹¤**: Microsoft Qlib ê¸°ë°˜ AI ì•ŒíŒŒ ëª¨ë¸ë§
        """)
        
    with c2:
        st.markdown("#### ğŸ“¢ Market Briefing")
        if st.button("âœ¨ ì˜¤ëŠ˜ì˜ ì‹œí™© ë¸Œë¦¬í•‘ ìƒì„± (Gemini)"):
            with st.spinner("ë‰´ìŠ¤ì™€ ì‹œì„¸ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    prompt = "ì˜¤ëŠ˜ì˜ ë¯¸êµ­ ì¦ì‹œ ë° í•œêµ­ ì¦ì‹œ ì£¼ìš” ì´ìŠˆë¥¼ 3ì¤„ë¡œ ìš”ì•½í•´ì¤˜."
                    summary = generate_content_with_rotation(prompt)
                    st.success(summary)
                except Exception as e:
                    st.error(f"ìƒì„± ì‹¤íŒ¨: {e}")

if selection == "ğŸ¤– ì±—ë´‡":
    st.title("ğŸ¤– ë¡œë³´ì–´ë“œë°”ì´ì € ìƒë‹´")
    st.caption("FAQ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ AIê°€ ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤.")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì¶”ì²œ ì§ˆë¬¸ (FAQ) ì˜ì—­ - ëŒ€í™” ê¸°ë¡ ì•„ë˜ì— ë°°ì¹˜
    # ëª…í™•í•œ í‚¤ì›Œë“œë¡œ ì§ì ‘ ì •ì˜
    faq_keywords = [
        "ì„œë¹„ìŠ¤ ê°€ì…/ì„¤ê³„",
        "í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘ ìˆ˜ì •",
        "í‡´ì§ì—°ê¸ˆ ê°€ì…ì œí•œ",
        "ê°œì¸ì—°ê¸ˆ ê°€ì…ì œí•œ",
        "ë§¤ë§¤/ë¦¬ë°¸ëŸ°ì‹± ê·œì¹™",
        "ìˆ˜ìµë¥  ë¯¸ë…¸ì¶œ ì‚¬ìœ ",
        "ì£¼ìš” ì—ëŸ¬ ì‚¬ë¡€"
    ]

    with st.expander("ğŸ’¡ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (ì¶”ì²œ í‚¤ì›Œë“œ)"):
        st.caption("ê¶ê¸ˆí•œ ë‚´ìš©ì„ í´ë¦­í•´ë³´ì„¸ìš”.")
        cols = st.columns(4) # 4ì—´ë¡œ ë°°ì¹˜
        for i, keyword in enumerate(faq_keywords):
            if cols[i % 4].button(keyword, key=f"faq_{i}"):
                st.session_state.messages.append({"role": "user", "content": f"{keyword}ì— ëŒ€í•´ ì•Œë ¤ì¤˜"})
                st.rerun()
            
    # ê°€ì¥ ìµœê·¼ ë©”ì‹œì§€ê°€ userì´ê³  assistantì˜ ë‹µë³€ì´ ì—†ì„ ë•Œ (ë²„íŠ¼ í´ë¦­ ì§í›„) ë‹µë³€ ìƒì„± íŠ¸ë¦¬ê±°
    if st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
        # ì´ë¯¸ ë‹µë³€ì´ ë‹¬ë¦° ì ì´ ìˆëŠ”ì§€ í™•ì¸ (ë§ˆì§€ë§‰ì´ userë©´ ë‹µë³€í•´ì•¼ í•¨)
        # í•˜ì§€ë§Œ Streamlit êµ¬ì¡°ìƒ chat_input ë£¨í”„ ë°–ì—ì„œ ì²˜ë¦¬í•´ì•¼ ìì—°ìŠ¤ëŸ¬ì›€.
        # ì—¬ê¸°ì„œëŠ” chat_inputì´ ì•„ë˜ì— ìˆì–´ì„œ, ë²„íŠ¼ í´ë¦­ -> rerun -> ì—¬ê¸°ê¹Œì§€ ì˜´ -> 
        # í™”ë©´ì— user msg í‘œì‹œë¨ -> ì´ì œ assistant msg í‘œì‹œí•  ì°¨ë¡€.
        
        # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ assistantê°€ ì•„ë‹ ê²½ìš°ì—ë§Œ ë‹µë³€ ìƒì„± ì‹œë„
        # (ì£¼ì˜: chat_inputì„ í†µí•œ ì…ë ¥ì€ ì•„ë˜ ë¸”ë¡ì—ì„œ ì²˜ë¦¬ë˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ë²„íŠ¼ í´ë¦­ìœ¼ë¡œ ì¸í•œ ê²½ìš°ë§Œ ì²˜ë¦¬í•˜ë©´ ì¢‹ìŒ.
        #  ê·¸ëŸ¬ë‚˜ ê°„ë‹¨í•˜ê²Œì§ì „ ë©”ì‹œì§€ê°€ userë©´ ë¬´ì¡°ê±´ ë‹µë³€í•˜ê²Œ ë¡œì§ì„ í†µí•©í•˜ëŠ”ê²Œ ê¹”ë”í•¨.
        #  ë‹¤ë§Œ ì•„ë˜ chat_input ë¡œì§ê³¼ ì¤‘ë³µë˜ì§€ ì•Šê²Œ í•´ì•¼ í•¨.)
        pass 

    # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
    if prompt := st.chat_input("ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

    # ë‹µë³€ ìƒì„± ë¡œì§ (ë²„íŠ¼ í´ë¦­ or ì…ë ¥ì°½ ì…ë ¥ ê³µí†µ ì²˜ë¦¬)
    if st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
        with st.chat_message("assistant"):
            try:
                # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
                last_user_msg = st.session_state.messages[-1]["content"]
                # API Key Rotation ì ìš©
                full_prompt = f"ì§ˆë¬¸: {last_user_msg}\n\në‹µë³€ (í•œêµ­ì–´ë¡œ, ê¸ˆìœµ ì „ë¬¸ê°€ì²˜ëŸ¼):"
                response_text = generate_content_with_rotation(full_prompt)
                
                st.markdown(response_text)
                st.session_state.messages.append({"role": "assistant", "content": response_text})
                
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

if selection == "ğŸ“„ Macro Talking Point":
    st.title("ğŸ“„ Macro Talking Point")
    st.caption("ê° ì§€ìˆ˜ì™€ ë‚ ì§œë³„ ë¦¬í¬íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    
    # ë¹„ë°€ë²ˆí˜¸ ë³´í˜¸ ê¸°ëŠ¥ ì¶”ê°€
    input_password = st.text_input("ì ‘ê·¼ ì•”í˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    
    # Secretsì—ì„œ ì„¤ì •í•œ ë¹„ë°€ë²ˆí˜¸ì™€ ë¹„êµ
    correct_password = st.secrets["MACRO_PAGE_PASSWORD"]
    
    if input_password != correct_password:
        st.warning("ğŸ”’ ì˜¬ë°”ë¥¸ ì•”í˜¸ë¥¼ ì…ë ¥í•´ì•¼ ë‚´ìš©ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        st.stop()
        
    st.success("ğŸ”“ ì¸ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # -----------------------------------------------------------------------------
    # HTML ë¦¬í¬íŠ¸ ë·°ì–´ (Iframe ë°©ì‹)
    # -----------------------------------------------------------------------------
    # ì§€ì •ëœ ë””ë ‰í† ë¦¬ ë‚´ì˜ ëª¨ë“  HTML íŒŒì¼ì„ ì°¾ì•„ì„œ ëª©ë¡ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.
    # NOTE: The original code used glob.glob and a specific naming convention.
    # The provided snippet suggests a different directory and naming convention.
    # For this edit, I will assume the user wants to keep the original report loading
    # mechanism but add the password protection.
    # If the user intended to replace the report loading logic, a separate instruction
    # would be needed.
    
    # ë¦¬í¬íŠ¸ íŒŒì¼ ìŠ¤ìº” í•¨ìˆ˜
    def get_reports():
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ html íŒŒì¼ ê²€ìƒ‰
        files = glob.glob("Macro Talking Point_ *.html")
        reports = []
        for f in files:
            # íŒŒì¼ëª… íŒŒì‹±: "Macro Talking Point_ {Index}_{Date}.html"
            # ì˜ˆ: "Macro Talking Point_ CPI_20251216.html"
            match = re.search(r"Macro Talking Point_ (.+?)_(\d+)\.html", f)
            if match:
                index_name = match.group(1)
                date_str = match.group(2)
                reports.append({
                    "filename": f,
                    "index": index_name,
                    "date": date_str,
                    "display": f"[{date_str}] {index_name}"
                })
        
        # ë‚ ì§œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        reports.sort(key=lambda x: x["date"], reverse=True)
        return reports


    reports = get_reports()

    if not reports:
        st.warning("í‘œì‹œí•  ë¦¬í¬íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # ë„¤ë¹„ê²Œì´ì…˜(ë¦¬í¬íŠ¸ ëª©ë¡)ì„ ì‚¬ì´ë“œë°”ì— ë°°ì¹˜í•˜ì—¬ ìŠ¤í¬ë¡¤ ì‹œì—ë„ ê³ ì •ë˜ë„ë¡ ë³€ê²½
        with st.sidebar:
            st.divider() # ë©”ë‰´ì™€ êµ¬ë¶„ì„ 
            st.markdown("### ğŸ“‘ ë¦¬í¬íŠ¸ ëª©ë¡")
            
            # 1. ì¹´í…Œê³ ë¦¬ í•„í„°ë§
            categories = sorted(list(set([r["index"] for r in reports])))
            categories.insert(0, "All")
            
            selected_category = st.selectbox("ì¹´í…Œê³ ë¦¬ ì„ íƒ:", categories)
            
            # ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì— ë”°ë¼ ë¦¬í¬íŠ¸ í•„í„°ë§
            if selected_category == "All":
                filtered_reports = reports
            else:
                filtered_reports = [r for r in reports if r["index"] == selected_category]
            
            # 2. ë¦¬í¬íŠ¸ ì„ íƒ
            if not filtered_reports:
                st.info("í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                selected_report = None
            else:
                report_options = [r["display"] for r in filtered_reports]
                selected_option = st.radio("ë³´ê³  ì‹¶ì€ ë¦¬í¬íŠ¸ë¥¼ ì„ íƒí•˜ì„¸ìš”:", report_options)
                
                # ì„ íƒëœ ë¦¬í¬íŠ¸ ì •ë³´ ì°¾ê¸°
                selected_report = next((r for r in reports if r["display"] == selected_option), None)

        if selected_report:
            # 1. ìŠ¤í¬ë¡¤ ì•µì»¤ ì‚½ì… (ì´ ìœ„ì¹˜ë¡œ ìŠ¤í¬ë¡¤ì„ ë•¡ê²¨ì˜¬ ì˜ˆì •)
            st.markdown('<div id="scroll-to-top-anchor"></div>', unsafe_allow_html=True)
            
            # ë¦¬í¬íŠ¸ ë³€ê²½ ì‹œ ìŠ¤í¬ë¡¤ì„ ë§¨ ìœ„ë¡œ ì´ˆê¸°í™” (JS Injection)
            current_report_key = selected_report["filename"]
            if "last_viewed_report" not in st.session_state:
                st.session_state["last_viewed_report"] = None

            if st.session_state["last_viewed_report"] != current_report_key:
                st.session_state["last_viewed_report"] = current_report_key
                components.html(
                    f"""
                    <script>
                        // ë¦¬í¬íŠ¸ í‚¤ê°€ ë°”ë€” ë•Œë§ˆë‹¤ ì‹¤í–‰: {current_report_key}
                        // ì•µì»¤(scroll-to-top-anchor)ë¥¼ ì°¾ì•„ì„œ scrollIntoView() í˜¸ì¶œ
                        // ë Œë”ë§ íƒ€ì´ë° ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ ë‹¤ì¤‘ ì‹œë„ (Burst)
                        function forceScroll() {{
                            try {{
                                var anchor = window.parent.document.getElementById("scroll-to-top-anchor");
                                if (anchor) {{
                                    anchor.scrollIntoView({{behavior: 'auto', block: 'start'}});
                                }}
                            }} catch(e) {{}}
                        }}
                        
                        // ì‹œë„ 1: ì¦‰ì‹œ
                        forceScroll(); 
                        // ì‹œë„ 2: 0.3ì´ˆ í›„ (DOM ë Œë”ë§ ì™„ë£Œ ì˜ˆìƒ)
                        setTimeout(forceScroll, 300);
                        // ì‹œë„ 3: 0.8ì´ˆ í›„ (í˜¹ì‹œ ëŠ¦ê²Œ ë¡œë”©ë  ê²½ìš°)
                        setTimeout(forceScroll, 800);
                    </script>
                    """,
                    height=0,
                    width=0
                )

            st.markdown(f"### ğŸ“‘ {selected_report['index']} ({selected_report['date']})")
            
            try:
                with open(selected_report["filename"], "r", encoding="utf-8") as f:
                    html_content = f.read()
                
                # ë†’ì´ ê³„ì‚° ë¡œì§ ê°œì„  (ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ íŠœë‹)
                # HTML íƒœê·¸ë“¤ì´ ë§ìœ¼ë¯€ë¡œ ë¼ì¸ ìˆ˜ * 15px ì •ë„ë¡œ ì¶•ì†Œ ê³„ì‚° (ê¸°ì¡´ 25px -> 15px)
                line_count = len(html_content.splitlines())
                
                # ë¼ì¸ ìˆ˜ê°€ ë„ˆë¬´ ì ìœ¼ë©´(minified) ê¸°ë³¸ ë†’ì´ ë¶€ì—¬, ì•„ë‹ˆë©´ ë¼ì¸ ìˆ˜ ë¹„ë¡€
                if line_count < 50:
                    estimated_height = 1200
                else:
                    estimated_height = max(800, line_count * 15 + 50)

                components.html(html_content, height=estimated_height, scrolling=True)
                
            except Exception as e:
                st.error(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

if selection == "ğŸ“ˆ ì „ëµ ì‹¤í—˜ì‹¤ (Beta)":
    st.title("ğŸ“ˆ ë‚˜ë§Œì˜ ì£¼ì‹ ì „ëµ ì‹¤í—˜ì‹¤ (Beta)")
    st.caption("ëŒ€í‘œì ì¸ íˆ¬ì ì „ëµë“¤ì„ ë‚´ ì…ë§›ëŒ€ë¡œ ì„¤ì •í•´ì„œ ê²€ì¦í•´ë³´ì„¸ìš”.")

    # 1. ì„¤ì • ì…ë ¥
    with st.expander("âš™ï¸ ë°±í…ŒìŠ¤íŒ… ì„¤ì •", expanded=True):
        col1, col2, col3 = st.columns(3)
        with col1:
            ticker_input = st.text_input("ì¢…ëª© ì½”ë“œ (ì—¬ëŸ¬ ê°œëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„)", value="SPY, QQQ, AAPL")
        with col2:
            start_date = st.date_input("ì‹œì‘ì¼", value=pd.to_datetime("2023-01-01"))
        with col3:
            end_date = st.date_input("ì¢…ë£Œì¼", value=pd.to_datetime("today"))

    st.divider()

    # 2. ì „ëµ ì„ íƒ ë° íŒŒë¼ë¯¸í„° ì„¤ì •
    st.subheader("ğŸ› ï¸ ì „ëµ êµ¬ì„±í•˜ê¸°")
    
    strategy_type = st.selectbox(
        "ì‚¬ìš©í•  ì „ëµì„ ì„ íƒí•˜ì„¸ìš”.",
        ["ì´ë™í‰ê· ì„  í¬ë¡œìŠ¤ (MA Crossover)", "RSI (ìƒëŒ€ê°•ë„ì§€ìˆ˜)", "ë³¼ë¦°ì € ë°´ë“œ (Bollinger Bands)"]
    )

    # ì „ëµë³„ íŒŒë¼ë¯¸í„° UI (ë™ì  ë³€ê²½)
    params = {}
    if strategy_type == "ì´ë™í‰ê· ì„  í¬ë¡œìŠ¤ (MA Crossover)":
        st.info("ğŸ’¡ **ê³¨ë“ í¬ë¡œìŠ¤ ì „ëµ**: ë‹¨ê¸° ì´í‰ì„ ì´ ì¥ê¸° ì´í‰ì„ ì„ ëŒíŒŒí•˜ë©´ ë§¤ìˆ˜, ê¹¨ì§€ë©´ ë§¤ë„í•©ë‹ˆë‹¤.")
        col_p1, col_p2 = st.columns(2)
        with col_p1:
            params['short_window'] = st.number_input("ë‹¨ê¸° ì´ë™í‰ê·  (ì¼)", value=20, min_value=1)
        with col_p2:
            params['long_window'] = st.number_input("ì¥ê¸° ì´ë™í‰ê·  (ì¼)", value=60, min_value=1)

    elif strategy_type == "RSI (ìƒëŒ€ê°•ë„ì§€ìˆ˜)":
        st.info("ğŸ’¡ **RSI ì—­ì¶”ì„¸ ì „ëµ**: ê³¼ë§¤ë„ êµ¬ê°„(ë§¤ìˆ˜ ê¸°ì¤€ ë¯¸ë§Œ)ì—ì„œ ë§¤ìˆ˜í•˜ê³ , ê³¼ë§¤ìˆ˜ êµ¬ê°„(ë§¤ë„ ê¸°ì¤€ ì´ˆê³¼)ì—ì„œ ë§¤ë„í•©ë‹ˆë‹¤.")
        col_p1, col_p2, col_p3 = st.columns(3)
        with col_p1:
            params['window'] = st.number_input("RSI ê¸°ê°„", value=14, min_value=1)
        with col_p2:
            params['buy_threshold'] = st.number_input("ë§¤ìˆ˜ ê¸°ì¤€ (ê³¼ë§¤ë„)", value=30, min_value=0, max_value=100)
        with col_p3:
            params['sell_threshold'] = st.number_input("ë§¤ë„ ê¸°ì¤€ (ê³¼ë§¤ìˆ˜)", value=70, min_value=0, max_value=100)

    elif strategy_type == "ë³¼ë¦°ì € ë°´ë“œ (Bollinger Bands)":
        st.info("ğŸ’¡ **ë³¼ë¦°ì € ë°´ë“œ ì „ëµ**: ì£¼ê°€ê°€ í•˜ë‹¨ ë°´ë“œë¥¼ í„°ì¹˜í•˜ë©´ ë§¤ìˆ˜, ìƒë‹¨ ë°´ë“œë¥¼ í„°ì¹˜í•˜ë©´ ë§¤ë„í•©ë‹ˆë‹¤ (í‰ê·  íšŒê·€).")
        col_p1, col_p2 = st.columns(2)
        with col_p1:
            params['window'] = st.number_input("ì´ë™í‰ê·  ê¸°ê°„", value=20, min_value=1)
        with col_p2:
            params['std_dev'] = st.number_input("í‘œì¤€í¸ì°¨ ìŠ¹ìˆ˜ (Standard Deviation multiplier)", value=2.0, step=0.1)

    # 3. ì „ëµ ì‹¤í–‰ ë¡œì§
    if st.button("ğŸš€ ì „ëµ ë¶„ì„ ì‹¤í–‰"):
        with st.spinner("ë°ì´í„° ë¶„ì„ ë° ì „ëµ ì‹œë®¬ë ˆì´ì…˜ ì¤‘..."):
            # ì…ë ¥ëœ í‹°ì»¤ íŒŒì‹± (ì‰¼í‘œ êµ¬ë¶„)
            tickers = [t.strip().upper() for t in ticker_input.split(',') if t.strip()]
            
            if not tickers:
                st.warning("ì¢…ëª© ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                st.stop()
            
            results_list = []
            equity_curves = pd.DataFrame()
            
            # ì§„í–‰ìƒí™©ë°”
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # SPY ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì‹œì¥ ìˆ˜ìµë¥  ë¹„êµìš©)
            spy_total_return = 0.0
            try:
                spy_df = yf.download("SPY", start=start_date, end=end_date, progress=False)
                if not spy_df.empty:
                    if isinstance(spy_df.columns, pd.MultiIndex):
                        spy_df.columns = spy_df.columns.get_level_values(0)
                    
                    if 'Adj Close' not in spy_df.columns:
                         if 'Close' in spy_df.columns:
                            spy_df['Adj Close'] = spy_df['Close']
                    
                    if 'Adj Close' in spy_df.columns:
                        spy_return_series = spy_df['Adj Close'].pct_change()
                        spy_cum_return = (1 + spy_return_series).cumprod()
                        spy_total_return = spy_cum_return.iloc[-1] - 1
            except Exception as e:
                st.warning(f"SPY ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

            for i, ticker in enumerate(tickers):
                status_text.text(f"ë¶„ì„ ì¤‘: {ticker} ({i+1}/{len(tickers)})")
                try:
                    # A. ë°ì´í„° ë‹¤ìš´ë¡œë“œ
                    df = yf.download(ticker, start=start_date, end=end_date, progress=False)
                    
                    if df.empty:
                        st.warning(f"'{ticker}': ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue
                    
                    # yfinance ìµœì‹  ë²„ì „ í˜¸í™˜ì„±
                    if isinstance(df.columns, pd.MultiIndex):
                        try:
                            df.columns = df.columns.get_level_values(0)
                        except:
                            pass

                    # ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬
                    if 'Adj Close' not in df.columns:
                        if 'Close' in df.columns:
                            df['Adj Close'] = df['Close']
                        else:
                            st.warning(f"'{ticker}': ê°€ê²© ë°ì´í„° ë¶€ì¡±. ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue
                    
                    # ìˆ˜ìµë¥  ê³„ì‚°
                    df = df.copy() # ê²½ê³  ë°©ì§€
                    df['Return'] = df['Adj Close'].pct_change()
                    df.dropna(inplace=True)
                    
                    # B. ì „ëµ ë¡œì§ ê³„ì‚°
                    df['Signal'] = 0 

                    # ---------------- [ì „ëµ í•¨ìˆ˜ ì ìš©] ----------------
                    if strategy_type == "ì´ë™í‰ê· ì„  í¬ë¡œìŠ¤ (MA Crossover)":
                        df['MA_Short'] = df['Adj Close'].rolling(window=params['short_window']).mean()
                        df['MA_Long'] = df['Adj Close'].rolling(window=params['long_window']).mean()
                        df.loc[df['MA_Short'] > df['MA_Long'], 'Signal'] = 1
                    
                    elif strategy_type == "RSI (ìƒëŒ€ê°•ë„ì§€ìˆ˜)":
                        delta = df['Adj Close'].diff()
                        gain = (delta.where(delta > 0, 0)).rolling(window=params['window']).mean()
                        loss = (-delta.where(delta < 0, 0)).rolling(window=params['window']).mean()
                        rs = gain / loss
                        df['RSI'] = 100 - (100 / (1 + rs))
                        
                        import numpy as np
                        df['Signal'] = np.nan
                        df.loc[df['RSI'] < params['buy_threshold'], 'Signal'] = 1
                        df.loc[df['RSI'] > params['sell_threshold'], 'Signal'] = 0
                        df['Signal'] = df['Signal'].ffill().fillna(0)

                    elif strategy_type == "ë³¼ë¦°ì € ë°´ë“œ (Bollinger Bands)":
                        df['MA'] = df['Adj Close'].rolling(window=params['window']).mean()
                        df['Std'] = df['Adj Close'].rolling(window=params['window']).std()
                        df['Upper'] = df['MA'] + (df['Std'] * params['std_dev'])
                        df['Lower'] = df['MA'] - (df['Std'] * params['std_dev'])
                        
                        import numpy as np
                        df['Signal'] = np.nan
                        df.loc[df['Adj Close'] < df['Lower'], 'Signal'] = 1
                        df.loc[df['Adj Close'] > df['Upper'], 'Signal'] = 0
                        df['Signal'] = df['Signal'].ffill().fillna(0)
                    # ------------------------------------------------

                    # C. ì„±ê³¼ ê³„ì‚°
                    df['Strategy_Return'] = df['Signal'].shift(1) * df['Return']
                    df['Cumulative_Strategy'] = (1 + df['Strategy_Return'].fillna(0)).cumprod()
                    df['Cumulative_Market'] = (1 + df['Return']).cumprod()
                    
                    # MDD
                    drawdown = df['Cumulative_Strategy'] / df['Cumulative_Strategy'].cummax() - 1
                    mdd = drawdown.min()
                    
                    # ìµœì¢… ìˆ˜ìµë¥ 
                    total_return = df['Cumulative_Strategy'].iloc[-1] - 1
                    market_return = df['Cumulative_Market'].iloc[-1] - 1 # Buy & Hold return
                    
                    # Alpha vs SPY
                    alpha_spy = total_return - spy_total_return

                    # ê²°ê³¼ ì €ì¥
                    results_list.append({
                        "ì¢…ëª©": ticker,
                        "ì „ëµ ìˆ˜ìµë¥ ": f"{total_return:.2%}",
                        "ìì²´ B&H": f"{market_return:.2%}", # Buy and Hold
                        "SPY ìˆ˜ìµë¥ ": f"{spy_total_return:.2%}",
                        "Alpha(vs SPY)": f"{alpha_spy:.2%}",
                        "MDD": f"{mdd:.2%}",
                        "Raw_Return": total_return # ì •ë ¬ìš©
                    })
                    
                    # ì°¨íŠ¸ìš© ë°ì´í„° (ì¸ë±ìŠ¤ í†µì¼)
                    equity_curves[ticker] = df['Cumulative_Strategy']
                
                except Exception as e:
                    st.warning(f"'{ticker}' ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress_bar.progress((i + 1) / len(tickers))
            
            status_text.empty()
            progress_bar.empty()

            if results_list:
                st.success(f"ì´ {len(results_list)}ê°œ ì¢…ëª© ë¶„ì„ ì™„ë£Œ!")
                
                # 1. ìš”ì•½ í…Œì´ë¸” (ìˆ˜ìµë¥  ìˆœ ì •ë ¬)
                results_df = pd.DataFrame(results_list)
                results_df = results_df.sort_values(by="Raw_Return", ascending=False).drop(columns=["Raw_Return"])
                
                st.subheader("ğŸ“Š ì¢…ëª©ë³„ ì„±ê³¼ (ìˆ˜ìµë¥  ìˆœ)")
                st.caption(f"SPY(S&P 500) ìˆ˜ìµë¥  ({start_date} ~ {end_date}): **{spy_total_return:.2%}**")
                st.dataframe(results_df, use_container_width=True)
                
                # 2. ë¹„êµ ì°¨íŠ¸
                st.subheader("ğŸ“ˆ ìˆ˜ìµë¥  ë¹„êµ ì°¨íŠ¸")
                if not equity_curves.empty:
                    # ì¸ë±ìŠ¤(ë‚ ì§œ)ê°€ ì„œë¡œ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ fillna
                    equity_curves = equity_curves.fillna(method='ffill').fillna(1.0)
                    fig = px.line(equity_curves, title=f"ì „ëµ ëˆ„ì  ìˆ˜ìµë¥  ë¹„êµ ({strategy_type})")
                    st.plotly_chart(fig, use_container_width=True)
            else:
                st.error("ë¶„ì„ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. í‹°ì»¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

elif selection == "ğŸ¤– AI ëª¨ë¸ í…ŒìŠ¤íŒ…":
    st.title("ğŸ¤– AI íŠ¸ë ˆì´ë”© ëª¨ë¸ ì—°êµ¬ì†Œ")
    st.caption("ê³¼ê±° ë°ì´í„°ë¡œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ë¯¸ë˜ ìˆ˜ìµë¥ ì„ ì˜ˆì¸¡í•˜ê³  ê²€ì¦í•©ë‹ˆë‹¤.")

    # Session State ì´ˆê¸°í™”
    if 'trained_models' not in st.session_state:
        st.session_state.trained_models = {}
    if 'gemini_insights' not in st.session_state:
        st.session_state.gemini_insights = {}

    # 1. ì„¤ì •
    with st.expander("âš™ï¸ ëª¨ë¸ë§ ì„¤ì •", expanded=True):
        col1, col2 = st.columns(2)
        with col1:
            universe_preset = st.selectbox(
                "ë¶„ì„ ëŒ€ìƒ ìœ ë‹ˆë²„ìŠ¤", 
                ["ì§ì ‘ ì…ë ¥", "NASDAQ Top 10 (Demo)", "Tech Giants (M7)", "NASDAQ Top 30 (Big Tech)", "S&P 500 Top 50 (Sector Leaders)", "NASDAQ 100 + S&P 500 (Market Proxy)"]
            )
            
            if universe_preset == "ì§ì ‘ ì…ë ¥":
                tickers_input = st.text_input("ì¢…ëª© ì½”ë“œ ì…ë ¥ (ì‰¼í‘œ êµ¬ë¶„)", "AAPL, MSFT, GOOGL, AMZN, NVDA")
                tickers = [t.strip().upper() for t in tickers_input.split(",") if t.strip()]
            
            elif universe_preset == "Tech Giants (M7)":
                tickers = ["AAPL", "MSFT", "GOOGL", "AMZN", "NVDA", "META", "TSLA"]
            
            elif universe_preset == "NASDAQ Top 10 (Demo)":
                tickers = ["AAPL", "MSFT", "NVDA", "GOOGL", "AMZN", "META", "TSLA", "AVGO", "COST", "PEP"]

            elif universe_preset == "NASDAQ Top 30 (Big Tech)":
                # ì‹œê°€ì´ì•¡ ìƒìœ„ ë“± ì£¼ìš” 30ê°œ ì¢…ëª©
                tickers = [
                    "AAPL", "MSFT", "NVDA", "GOOGL", "AMZN", "META", "TSLA", "AVGO", "COST", "PEP",
                    "CSCO", "NFLX", "AMD", "ADBE", "TMUS", "INTC", "QCOM", "TXN", "AMGN", "HON",
                    "AMAT", "INTU", "SBUX", "ADP", "BKNG", "GILD", "ISRG", "MDLZ", "REGN", "VRTX"
                ]
            
            elif universe_preset == "S&P 500 Top 50 (Sector Leaders)":
                # S&P 500 ì£¼ìš” ì¢…ëª© 50ê°œ (ì˜ˆì‹œ)
                tickers = [
                    "AAPL", "MSFT", "NVDA", "GOOGL", "AMZN", "META", "TSLA", "BRK-B", "LLY", "V",
                    "TSM", "UNH", "XOM", "JPM", "JNJ", "WMT", "MA", "PG", "HD", "AVGO", 
                    "CVX", "MRK", "ABBV", "COST", "PEP", "KO", "ADBE", "BAC", "CSCO", "CRM",
                    "MCD", "TMO", "ACN", "NFLX", "AMD", "LIN", "ABT", "DHR", "DIS", "NKE",
                    "WFC", "TXN", "NEE", "PM", "VZ", "RTX", "INTC", "QCOM", "UPS", "HON"
                ]

            elif universe_preset == "NASDAQ 100 + S&P 500 (Market Proxy)":
                # Option A: Market Proxy (Speed) vs Option B: Full Universe (Sloooow)
                scan_mode = st.radio("ë¶„ì„ ëª¨ë“œ ì„ íƒ", ["ğŸš€ ì†ë„ ìš°ì„  (Top 100 Market Proxy)", "ğŸ¢ ì •ë°€ ë¶„ì„ (S&P 500 ì „ì¢…ëª© / ~5 min)"], horizontal=True)
                
                if "ì†ë„ ìš°ì„ " in scan_mode:
                    # Proxy for full market: NASDAQ 100 constituents (approx) + Key S&P 500
                    # ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì²œ ê°œë¥¼ ë‹¤ ë°›ìœ¼ë©´ ë„ˆë¬´ ëŠë¦¬ë¯€ë¡œ, ëŒ€í‘œ ìš°ëŸ‰ì£¼ ~100ê°œë¡œ êµ¬ì„±ëœ Proxy ì‚¬ìš©
                    # ì‚¬ìš©ìê°€ ìš”ì²­í•œ 'ì „ì²´' ëŠë‚Œì„ ë‚´ê¸° ìœ„í•´ ì„¹í„°ë³„ ëŒ€í‘œì£¼ë¥¼ ìµœëŒ€í•œ ë§ì´ í¬í•¨
                    tickers = [
                        # Tech
                        "AAPL", "MSFT", "NVDA", "GOOGL", "AMZN", "META", "TSLA", "AVGO", "ADBE", "CRM", "AMD", "INTC", "QCOM", "TXN", "IBM", "ORCL", "CSCO", "MU", "LRCX", "AMAT",
                        # Finance
                        "JPM", "BAC", "WFC", "C", "GS", "MS", "BLK", "AXP", "V", "MA", "PYPL", "BRK-B", "SPGI",
                        # Health
                        "LLY", "UNH", "JNJ", "MRK", "ABBV", "PFE", "TMO", "ABT", "DHR", "BMY", "AMGN", "GILD", "ISRG", "VRTX", "REGN",
                        # Consumer
                        "AMZN", "TSLA", "HD", "MCD", "NKE", "SBUX", "WMT", "COST", "PG", "KO", "PEP", "PM", "MO", "CL", "EL",
                        # Industrial / Energy / etc
                        "XOM", "CVX", "COP", "SLB", "EOG", "CAT", "DE", "HON", "GE", "LMT", "RTX", "BA", "UPS", "FDX", "UNP", "NEE", "DUK", "SO",
                        # + NASDAQ 100 extras
                        "NFLX", "CMCSA", "TMUS", "CHTR", "BKNG", "ADP", "MDLZ", "CSX", "MAR", "CTAS", "KLAC", "SNPS", "CDNS", "PANW", "FTNT",
                        "MELI", "NXPI", "ORLY", "ROP", "ODFL", "PCAR", "MNST", "KDP", "EXC", "XEL", "IDXX", "BIIB", "MCHP", "ALGN", "DLTR"
                    ]
                    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
                    tickers = sorted(list(set(tickers)))
                    st.caption(f"â„¹ï¸ ì†ë„ ìµœì í™”ë¥¼ ìœ„í•´ ì£¼ìš” {len(tickers)}ê°œ ìš°ëŸ‰ì£¼ë¡œ ìœ ë‹ˆë²„ìŠ¤ë¥¼ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.")
                else:
                    # Full Mode: S&P 500 Constituents + NASDAQ 100 Full
                    try:
                        import FinanceDataReader as fdr
                        with st.spinner("S&P 500 ë° NASDAQ 100 ì „ì¢…ëª©ì„ ë³‘í•© ì¤‘ì…ë‹ˆë‹¤..."):
                            # 1. S&P 500 (Dynamic)
                            df_sp500 = fdr.StockListing('S&P500')
                            sp500_tickers = df_sp500['Symbol'].tolist()
                            
                            # 2. NASDAQ 100 (Static base + Dynamic merge)
                            # NASDAQ 100 ì¢…ëª©ë“¤ì€ ëŒ€ë¶€ë¶„ S&P 500ì— í¬í•¨ë˜ì§€ë§Œ, í¬í•¨ë˜ì§€ ì•ŠëŠ” ê²ƒë“¤ë„ ìˆìŒ (ì˜ˆ: ì¼ë¶€ ADR, ë¹„ë¯¸êµ­ê³„ ë“±)
                            # ë‘ ë¦¬ìŠ¤íŠ¸ë¥¼ í•©ì¹©ë‹ˆë‹¤.
                            
                            combined = list(set(sp500_tickers + NASDAQ_100_FULL))
                            tickers = combined
                            
                            # 500ê°œê°€ ë„˜ìœ¼ë¯€ë¡œ ì§„í–‰ ìƒí™©ì— ìœ ì˜í•˜ë¼ëŠ” ë©”ì‹œì§€
                            st.warning(f"âš ï¸ ì´ {len(tickers)}ê°œ ì¢…ëª© (S&P 500 + NASDAQ 100)ì„ ë¶„ì„í•©ë‹ˆë‹¤. ë°ì´í„° ë‹¤ìš´ë¡œë“œì— ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤. (3~5ë¶„ ì˜ˆìƒ)")
                    except Exception as e:
                        st.error(f"ì¢…ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
                        tickers = ["AAPL", "MSFT"] # Fallback

            if universe_preset != "ì§ì ‘ ì…ë ¥":
                st.info(f"ì„ íƒëœ ìœ ë‹ˆë²„ìŠ¤: {len(tickers)}ê°œ ì¢…ëª©")

        with col2:
            model_type = st.selectbox(
                "ì‚¬ìš©í•  AI ëª¨ë¸", 
                ["â­ ì•™ìƒë¸” (Ensemble: Linear+SVM+LGBM)", "Linear Regression (ì„ í˜•íšŒê·€)", "LightGBM (íŠ¸ë¦¬ ë¶€ìŠ¤íŒ…)", "SVM (Support Vector Machine)"]
            )
            
            # Prediction Horizon ( ë³´ìœ  ê¸°ê°„ )
            horizon_option = st.radio(
                "ì˜ˆì¸¡ ê¸°ê°„ (ë³´ìœ  ê¸°ê°„)",
                ["1 Day (ë‹¨ê¸° íŠ¸ë ˆì´ë”©)", "2 Weeks (ìŠ¤ìœ™ íŠ¸ë ˆì´ë”©)"],
                index=1
            )
            
            # Feature ë³µì¡ë„ ì„ íƒ
            feature_level = st.radio(
                "Feature ë³µì¡ë„ (AI ì§€ëŠ¥)", 
                ["Light (5ê°œ - ì†ë„ ì¤‘ì‹¬)", "Standard (22ê°œ - ê· í˜•)", "Rich (50+ê°œ - ì •ë°€ ë¶„ì„)", "Alpha158 (Qlib - Pro)"],
                index=1
            )
            
            # Top-K ì„ íƒ
            top_k_select = st.number_input("ì¶”ì²œí•  ì¢…ëª© ìˆ˜ (Top K)", min_value=1, max_value=20, value=10)
    
        col_d1, col_d2 = st.columns(2)
        with col_d1:
            train_start = st.date_input("í•™ìŠµ ì‹œì‘ì¼", pd.to_datetime("2020-01-01"))
        with col_d2:
            test_start = st.date_input("í…ŒìŠ¤íŠ¸ ì‹œì‘ì¼ (Backtest Start)", pd.to_datetime("2023-01-01"))

    # 2. ì‹¤í–‰ (í•™ìŠµ ë²„íŠ¼)
    if st.button("ğŸ§  AI ëª¨ë¸ í•™ìŠµ ì‹œì‘"):
        status_text = st.empty()
        progress_bar = st.progress(0)
        
        # A. ë°ì´í„° ìˆ˜ì§‘ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
        status_text.text("ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° í”¼ì²˜ ìƒì„± ì¤‘...")
        
        full_data = {}
        valid_tickers = []
        
        # ì „ì²´ ê¸°ê°„ ì„¤ì •
        end_date = pd.to_datetime("today")
        
        for i, ticker in enumerate(tickers):
            try:
                # ë„‰ë„‰í•˜ê²Œ ë°›ì•„ì„œ ì´í‰ì„  ê³„ì‚° (Rich ëª¨ë“œì¼ ê²½ìš° ë” ë§ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ)
                if "Alpha158" in feature_level:
                    lookback_days = 365 # Alpha158 requires long history
                else:
                    lookback_days = 200 if "Rich" in feature_level else 100
                df = yf.download(ticker, start=train_start - pd.Timedelta(days=lookback_days), end=end_date, progress=False)
                
                # MultiIndex ì²˜ë¦¬
                if isinstance(df.columns, pd.MultiIndex):
                    df.columns = df.columns.get_level_values(0)
                
                # ì»¬ëŸ¼ ë³´ì •
                if 'Adj Close' not in df.columns:
                    if 'Close' in df.columns:
                        df['Adj Close'] = df['Close']
                    else:
                        continue
                
                df = df[['Open', 'High', 'Low', 'Adj Close', 'Volume']].copy()
                df.columns = ['Open', 'High', 'Low', 'Close', 'Volume'] 
                
                # [Fix] Ensure no NaNs before feature calc (Alpha158 polyfit fails on NaNs)
                df.dropna(inplace=True)

                # ---------------- [Feature Engineering (Refactored)] ----------------
                df, feature_cols = calculate_feature_set(df, feature_level)

                # Label (Target): ë‹¤ìŒë‚  ìˆ˜ìµë¥  or 2ì£¼ í›„ ìˆ˜ìµë¥ 
                if "2 Weeks" in horizon_option:
                    # 10ê±°ë˜ì¼ í›„ì˜ ìˆ˜ìµë¥  (2ì£¼)
                    df['Next_Return'] = df['Close'].pct_change(10).shift(-10)
                else:
                    # 1ì¼ í›„ (ë‹¨ê¸°)
                    df['Next_Return'] = df['Close'].pct_change().shift(-1)
                
                df.dropna(inplace=True)
                
                if not df.empty:
                    full_data[ticker] = df
                    valid_tickers.append(ticker)
                    
            except Exception as e:
                # [Debug] Show error for first few tickers to diagnose
                if i < 3: st.warning(f"âš ï¸ {ticker} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                pass
            
            progress_bar.progress((i + 1) / len(tickers) * 0.3)

        if not valid_tickers:
            st.error("ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()
            
        # B. ëª¨ë¸ í•™ìŠµ
        status_text.text(f"{model_type} ëª¨ë¸ í•™ìŠµ ì¤‘ (Features: {len(feature_cols)}ê°œ)...")
        
        # ì „ì²´ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ í•™ìŠµì…‹ìœ¼ë¡œ ë³‘í•© (Global Model)
        X_train_all = []
        y_train_all = []
        
        # feature_colsëŠ” ìœ„ì—ì„œ ìë™ ìƒì„±ë¨
        
        test_datasets = {} 
        
        for ticker in valid_tickers:
            df = full_data[ticker]
            train_mask = df.index < pd.to_datetime(test_start)
            test_mask = df.index >= pd.to_datetime(test_start)
            
            train_df = df[train_mask]
            test_df = df[test_mask]
            
            if not train_df.empty:
                X_train_all.append(train_df[feature_cols].values)
                y_train_all.append(train_df['Next_Return'].values)
            
            if not test_df.empty:
                test_datasets[ticker] = test_df
        
        if not X_train_all:
            st.error("í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ê¸°ê°„ì„ ëŠ˜ë ¤ì£¼ì„¸ìš”.")
            st.stop()
            
        X_train = np.concatenate(X_train_all)
        y_train = np.concatenate(y_train_all)
        
        # Scaling
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        
        # Model Fitting
        if "Ensemble" in model_type:
             # ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ
            st.info("â­ ì•™ìƒë¸” ëª¨ë“œ: 3ê°€ì§€ ëª¨ë¸(Linear, LightGBM, SVM)ì„ ëª¨ë‘ í•™ìŠµí•©ë‹ˆë‹¤...")
            
            # 1. Linear
            model_lin = LinearRegression()
            model_lin.fit(X_train_scaled, y_train)
            
            # 2. LightGBM
            try:
                import lightgbm as lgb
                model_lgb = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.05, random_state=42)
                model_lgb.fit(X_train_scaled, y_train)
            except ImportError:
                st.warning("LightGBM not installed. Using Linear instead.")
                model_lgb = model_lin

            # 3. SVM (SVR)
            from sklearn.svm import SVR
            # ë°ì´í„°ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ SVRì€ ëŠë¦¼. ìƒ˜í”Œë§í•˜ê±°ë‚˜ LinearSVR ì‚¬ìš©
            if len(X_train) > 5000:
                from sklearn.svm import LinearSVR
                model_svr = LinearSVR(random_state=42, max_iter=1000)
            else:
                model_svr = SVR(kernel='rbf')
            
            model_svr.fit(X_train_scaled, y_train)
            
            # ì•™ìƒë¸”ì€ 3ê°œ ëª¨ë¸ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥
            model = {
                "Linear": model_lin,
                "LightGBM": model_lgb,
                "SVM": model_svr
            }

        elif "Linear" in model_type:
            model = LinearRegression()
            model.fit(X_train_scaled, y_train)
        elif "SVM" in model_type:
            if len(X_train) > 10000:
                st.warning("ë°ì´í„°ê°€ ë§ì•„ SVM í•™ìŠµ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            model = SVR(kernel='rbf', C=1.0, epsilon=0.1)
            model.fit(X_train_scaled, y_train)
        elif "LightGBM" in model_type:
            import lightgbm as lgb
            model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.05, num_leaves=31, random_state=42, verbose=-1)
            model.fit(X_train_scaled, y_train)
            
        progress_bar.progress(0.7)
        
        # C. ì˜ˆì¸¡ ë° ë°±í…ŒìŠ¤íŒ… (Dynamic Top-K)
        status_text.text(f"ë°±í…ŒìŠ¤íŒ… ì‹œë®¬ë ˆì´ì…˜ ì¤‘ (Top {top_k_select})...")
        
        # ì•™ìƒë¸” ì˜ˆì¸¡ í•¨ìˆ˜
        def predict_ensemble(models, X):
            p1 = models["Linear"].predict(X)
            p2 = models["LightGBM"].predict(X)
            p3 = models["SVM"].predict(X)
            # ë‹¨ìˆœ í‰ê· 
            return (p1 + p2 + p3) / 3

        all_test_dates = sorted(list(set().union(*[d.index for d in test_datasets.values()])))
        
        # Holding Period Check
        holding_period = 10 if "2 Weeks" in horizon_option else 1
        rebalance_dates = all_test_dates[::holding_period]
        
        # ì§„í–‰ìƒí™©ìš©
        total_steps = len(rebalance_dates) - 1
        
        cum_ret_model = 1.0
        cum_ret_bench = 1.0
        
        plot_dates = []
        plot_model = []
        plot_bench = []
        
        for i in range(total_steps):
            curr_date = rebalance_dates[i]
            next_date = rebalance_dates[i+1] # ë‹¤ìŒ ë¦¬ë°¸ëŸ°ì‹± ë‚ ì§œ
            
            # í˜„ì¬ ì‹œì  ë°ì´í„°ë¡œ ì˜ˆì¸¡
            candidates = []
            
            for ticker in valid_tickers:
                if ticker in test_datasets and curr_date in test_datasets[ticker].index:
                    df = test_datasets[ticker]
                    row = df.loc[curr_date]
                    
                    # Feature
                    feats = row[feature_cols].values.reshape(1, -1)
                    feats_scaled = scaler.transform(feats)
                    
                    # Score
                    if isinstance(model, dict): # Ensemble
                        score = predict_ensemble(model, feats_scaled)[0]
                    else:
                        score = model.predict(feats_scaled)[0]

                    # Actual Return (curr_date -> next_date)
                    actual_ret = 0.0
                    try:
                        p_start = df.loc[curr_date, 'Close']
                        # next_dateê°€ ì—†ìœ¼ë©´ ê·¸ ë¯¸ë˜ ì–´ë”˜ê°€.. nearest?
                        # ë‹¨ìˆœí™”: next_dateê°€ ì¡´ì¬í•˜ë©´ ì”€. ì•„ë‹ˆë©´ ë§ˆì§€ë§‰.
                        if next_date in df.index:
                            p_end = df.loc[next_date, 'Close']
                        else:
                            # next_dateê°€ dfë²”ìœ„ë¥¼ ë²—ì–´ë‚  ìˆ˜ë„ ìˆìŒ (ê°œë³„ ì¢…ëª© ìƒí ë“±)
                            # rebalance_date logic is global, but individual ticker might end early.
                            sub_df = df.loc[curr_date:]
                            if not sub_df.empty:
                                p_end = sub_df.iloc[-1]['Close']
                            else:
                                p_end = p_start
                        
                        actual_ret = (p_end / p_start) - 1
                    except:
                        actual_ret = 0.0

                    candidates.append({
                        "ticker": ticker,
                        "score": score,
                        "ret": actual_ret
                    })
            
            if not candidates:
                continue
                
            # Score ê¸°ì¤€ ì •ë ¬
            candidates.sort(key=lambda x: x['score'], reverse=True)
            
            # Top-K
            picks = candidates[:top_k_select]
            
            # ìˆ˜ìµë¥  ê³„ì‚° (Equal Weight)
            raw_period_ret = sum([p['ret'] for p in picks]) / len(picks) if picks else 0.0
            
            # [Transaction Cost] 0.1% (0.001) per rebalance
            cost = 0.001
            period_ret = raw_period_ret - cost
            
            # Benchmark (Equal Weight of Universe)
            bench_ret = sum([p['ret'] for p in candidates]) / len(candidates) if candidates else 0.0
            
            # ëˆ„ì 
            cum_ret_model *= (1 + period_ret)
            cum_ret_bench *= (1 + bench_ret)
            
            plot_dates.append(next_date)
            plot_model.append(cum_ret_model)
            plot_bench.append(cum_ret_bench)
            
        progress_bar.progress(1.0)
        status_text.empty()
        
        # D. ê²°ê³¼ ì €ì¥ (Session State)
        st.session_state.trained_models[model_type] = {
            "model": model,
            "scaler": scaler,
            "feature_cols": feature_cols,
            "full_data": full_data,
            "valid_tickers": valid_tickers,
            "top_k": top_k_select,
            "feature_level": feature_level,
            "horizon": horizon_option # ì €ì¥
        }
        
        # E. ê²°ê³¼ ì‹œê°í™”
        # E. ê²°ê³¼ ì‹œê°í™” & SPY Benchmark
        # SPY ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        plot_spy = [1.0] * len(plot_dates)
        try:
            spy_df = yf.download("SPY", start=plot_dates[0], end=pd.to_datetime(plot_dates[-1]) + pd.Timedelta(days=5), progress=False)
            if isinstance(spy_df.columns, pd.MultiIndex): spy_df.columns = spy_df.columns.get_level_values(0)
            target_col = 'Adj Close' if 'Adj Close' in spy_df.columns else 'Close'
            
            valid_dt = spy_df.index.asof(plot_dates[0])
            if pd.notna(valid_dt):
                base_price = spy_df.loc[valid_dt, target_col]
                temp_spy = []
                for d in plot_dates:
                    v_dt = spy_df.index.asof(d)
                    if pd.notna(v_dt):
                        temp_spy.append(spy_df.loc[v_dt, target_col] / base_price)
                    else:
                        temp_spy.append(1.0)
                plot_spy = temp_spy
        except:
            pass

        results_df = pd.DataFrame({
            "Date": plot_dates,
            "Strategy (AI)": plot_model,
            "S&P 500 (SPY)": plot_spy,
            "Benchmark (Equal)": plot_bench
        }).set_index("Date")
        
        st.success(f"í•™ìŠµ ì™„ë£Œ! ({model_type}) - Horizon: {horizon_option}, Top-{top_k_select}")
        
        # Prepare Backtest Data Dict for Saving
        backtest_data_to_save = {}
        
        if results_df.empty:
            st.warning("âš ï¸ ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„ ë™ì•ˆ ë§¤ë§¤ ì‹ í˜¸ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ê±°ë‚˜ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ê²°ê³¼ ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            total_ret = results_df['Strategy (AI)'].iloc[-1] - 1
            spy_ret = results_df['S&P 500 (SPY)'].iloc[-1] - 1
            eq_ret = results_df['Benchmark (Equal)'].iloc[-1] - 1
            
            backtest_data_to_save = {
                "perf_df": results_df,
                "metrics": {
                    "Total Return": f"{total_ret:.2%}",
                    "SPY Return": f"{spy_ret:.2%}",
                    "EQ Return": f"{eq_ret:.2%}"
                }
            }

            c1, c2, c3 = st.columns(3)
            c1.metric("AI í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  (Cost 0.1%)", f"{total_ret:.2%}")
            c2.metric("S&P 500 ìˆ˜ìµë¥ ", f"{spy_ret:.2%}")
            c3.metric("ë™ì¼ ë¹„ì¤‘ (Equal)", f"{eq_ret:.2%}")
            
            st.subheader(f"ğŸ“ˆ ë°±í…ŒìŠ¤íŒ… ê²°ê³¼: AI Top-{top_k_select} ì „ëµ vs ì‹œì¥")
            # st.line_chart(results_df) # Moved to Tab View

        # [Persistence Save] (Executed regardless of backtest result)
        try:
            # ì•™ìƒë¸”ì€ ëª¨ë¸ êµ¬ì¡°ê°€ ë‹¤ë¥´ë¯€ë¡œ ì €ì¥ ë°©ì‹ ìœ ì˜
            model_data_to_save = {
                "model_type": model_type,
                "model": model,
                "scaler": scaler,
                "feature_cols": feature_cols,
                "feature_level": feature_level,
                "horizon": horizon_option,
                "top_k": top_k_select,
                "timestamp": pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
                "valid_tickers": valid_tickers,
                "backtest_data": backtest_data_to_save, # Save Performance
                "train_period": f"{train_start.strftime('%Y-%m-%d')} ~ {pd.to_datetime(test_start).strftime('%Y-%m-%d')}"
            }
            
            # Update Session State too
            st.session_state.trained_models[model_type] = model_data_to_save
            
            # íŒŒì¼ëª…: {Model}_{Horizon}_{Feat}_{TopK}_{Date}.pkl
            # [Fix] Aggressive Sanitization for Windows/Cloud Compatibility
            def sanitize_filename(s):
                # Remove emojis, special chars, keep alphanumeric, spaces, hyphens, underscores
                s = re.sub(r'[^\w\s-]', '', s) # Remove non-word except space/hyphen
                return s.replace(" ", "")
            
            safe_type = sanitize_filename(model_type)
            safe_horizon = sanitize_filename(horizon_option)
            safe_feat = feature_level.split(" ")[0] # Light, Standard, Rich
            today_str = pd.Timestamp.now().strftime('%Y-%m-%d')
            
            # [Added] Training Period in Filename (Year Only)
            period_str = f"{train_start.year}-{test_start.year}"
            
            file_name_ver = f"{safe_type}_{safe_horizon}_{safe_feat}_Top{top_k_select}_{period_str}_{today_str}"
            
            save_model_checkpoint(file_name_ver, model_data_to_save)
            st.toast(f"âœ… ëª¨ë¸ ìë™ ì €ì¥ ì™„ë£Œ: {file_name_ver}")
            
            # [Download Button for Git Persistence]
            saved_path = os.path.join(MODEL_SAVE_DIR, f"{file_name_ver}.pkl")
            
            st.divider() # Visual separation
            
            if os.path.exists(saved_path):
                # st.success(f"ëª¨ë¸ ì €ì¥ ì„±ê³µ: {saved_path}") # Debug feedback
                with open(saved_path, "rb") as f:
                    btn = st.download_button(
                        label=f"ğŸ“¥ ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (.pkl)\n({file_name_ver})",
                        data=f,
                        file_name=f"{file_name_ver}.pkl",
                        mime="application/octet-stream",
                        help="ì´ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ ë°›ì•„ GitHub 'saved_models' í´ë”ì— ì»¤ë°‹í•˜ë©´, Cloud í™˜ê²½ì—ì„œë„ ì˜êµ¬ ì €ì¥ë©ë‹ˆë‹¤.",
                        use_container_width=True # Make it prominent
                    )
            else:
                st.error(f"ì €ì¥ëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {saved_path}")
        except Exception as e:
            st.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    # [Fast Inference Button Logic]
    # ëª¨ë¸ í•™ìŠµ ë²„íŠ¼ ì˜†ì— 'ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°' ë²„íŠ¼ì´ ìˆìœ¼ë©´ ì¢‹ê² ì§€ë§Œ, UI ë ˆì´ì•„ì›ƒìƒ
    # '2. ì‹¤í–‰ (í•™ìŠµ ë²„íŠ¼)' ì•„ë˜ì— ì¡°ê±´ì„ ë‘ê±°ë‚˜ ë³‘ë ¬ë¡œ ë‘ .
    
    # Scan for existing saved models matching current selection
    # [Fix] Use same sanitization logic as Saving to match filenames
    def sanitize_filename_search(s):
        s = re.sub(r'[^\w\s-]', '', s)
        return s.replace(" ", "")

    # [User Feedback] Scan ALL saved models, not just the selected type
    # This allows loading a 'Linear' model even if Sidebar says 'Transformer'
    safe_type = sanitize_filename_search(model_type)
    
    # Original: valid_pattern = os.path.join(MODEL_SAVE_DIR, f"*{safe_type}*.pkl")
    # New: Show all
    search_pattern = os.path.join(MODEL_SAVE_DIR, "*.pkl")
    found_files = glob.glob(search_pattern)
    
    # [Fix] Also search for 'colab_' files regardless of type selection (Universal Fallback)
    colab_pattern = os.path.join(MODEL_SAVE_DIR, "colab_*.pkl")
    colab_files = glob.glob(colab_pattern)
    
    # Union of files (avoid duplicates)
    found_files = list(set(found_files + colab_files))
    
    loaded_model_data = None
    
    if found_files:
        # Sort by modification time (newest first)
        found_files.sort(key=os.path.getmtime, reverse=True)
        
        # Pretty names for Dropdown
        # 0. Find PyTorch Models (.pth + .json)
        # Search for pairs: {name}_weights.pth and {name}_config.json
        pth_files = glob.glob(os.path.join(MODEL_SAVE_DIR, "*_weights.pth"))
        pytorch_models = {}
        for p in pth_files:
            base = p.replace("_weights.pth", "")
            config_file = f"{base}_config.json"
            if os.path.exists(config_file):
                pytorch_models[os.path.basename(base)] = {'weights': p, 'config': config_file}

        # Combine options
        # Existing .pkl files (found_files already contains correctly sorted paths from lines 1569/1575)
        file_options = {}
        
        for f in found_files:
            fname = os.path.basename(f).replace(".pkl", "")
            file_options[fname] = {'type': 'pkl', 'path': f}
            
        # Add PyTorch models to the list
        for name, paths in pytorch_models.items():
            file_options[f"[PyTorch] {name}"] = {'type': 'pth', 'path': paths['weights'], 'config': paths['config']}

        st.info(f"ğŸ’¡ ì €ì¥ëœ ëª¨ë¸: {len(file_options)}ê°œ ë°œê²¬ë¨")
        selected_ver_key = st.selectbox("ğŸ“‚ ë¶ˆëŸ¬ì˜¬ ëª¨ë¸ ë²„ì „ ì„ íƒ", list(file_options.keys()))
        
        selected_ver = file_options.get(selected_ver_key)
        
        if selected_ver:
             try:
                 if selected_ver['type'] == 'pth':
                     # ---------------------------------------------------------
                     # [Safe] Load Pure PyTorch Model (No Qlib Required)
                     # ---------------------------------------------------------
                     import torch
                     import torch.nn as nn
                     import json
                     import math

                     st.toast("ğŸ›¡ï¸ ì•ˆì „ ëª¨ë“œ: ìˆœìˆ˜ PyTorch ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤.")

                     # 1. Load Config
                     with open(selected_ver['config'], 'r') as f:
                         conf = json.load(f)

                     # 2. Define Pure PyTorch Transformer Class (Indentical to Qlib's logic)
                     class PositionalEncoding(nn.Module):
                        def __init__(self, d_model, max_len=1000):
                            super(PositionalEncoding, self).__init__()
                            pe = torch.zeros(max_len, d_model)
                            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
                            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
                            pe[:, 0::2] = torch.sin(position * div_term)
                            pe[:, 1::2] = torch.cos(position * div_term)
                            # [Fix] Shape match: [1000, 1, 64]
                            self.register_buffer('pe', pe.unsqueeze(1))
                        def forward(self, x):
                            return x + self.pe[:x.size(0), :]

                     class TransformerModel(nn.Module):
                        def __init__(self, d_feat, d_model=64, nhead=8, num_layers=2, dropout=0.1, device='cpu'):
                            super(TransformerModel, self).__init__()
                            self.feature_layer = nn.Linear(d_feat, d_model)
                            self.pos_encoder = PositionalEncoding(d_model)
                            # [Fix] dim_feedforward=2048 (Default)
                            encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, 2048, dropout)
                            self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)
                            self.decoder_layer = nn.Linear(d_model, 1)
                            self.device = device
                            self._d_feat_val = d_feat
                        def forward(self, src):
                            src = self.feature_layer(src)
                            src = self.pos_encoder(src)
                            output = self.transformer_encoder(src)
                            output = self.decoder_layer(output[:, -1, :]) # Use last step
                            return output.squeeze()
                        # Add d_feat property for auto-detection
                        @property
                        def d_feat(self):
                            return self._d_feat_val
                        def __init__(self, d_feat, d_model=64, nhead=8, num_layers=2, dropout=0.1, device='cpu'):
                            super(TransformerModel, self).__init__()
                            self._d_feat_val = d_feat # Store for property
                        def predict(self, dataset):
                            # Adapter for DataFrame or Numpy
                            val = None
                            if isinstance(dataset, pd.DataFrame):
                                val = dataset.values
                            elif isinstance(dataset, np.ndarray):
                                val = dataset
                            
                            if val is not None:
                                x = torch.tensor(val, dtype=torch.float32).to(self.device)
                                if len(x.shape) == 2:
                                    x = x.unsqueeze(1) # [Batch, 1, Feat]
                                self.eval()
                                with torch.no_grad():
                                    pred = self.forward(x)
                                return pred.cpu().numpy()
                            return []

                     # 3. Initialize Model
                     model = TransformerModel(
                         d_feat=conf.get('d_feat', 20), 
                         d_model=conf.get('d_model', 64),
                         nhead=conf.get('nhead', 8),
                         num_layers=conf.get('num_layers', 2),
                         device='cpu' 
                     )
                     
                     # 4. Load Weights
                     # [Fix] strict=False to ignore 'encoder_layer.*' keys which are unused prototype layers in Qlib
                     model.load_state_dict(torch.load(selected_ver['path'], map_location=torch.device('cpu')), strict=False)
                     model.eval()
                     
                     # 5. Wrap as standard object
                     loaded_model_data = {
                         "model": model,
                         "feature_level": "Standard", # Default
                         "description": "PyTorch Transformer (Imported)",
                         "scaler": None, # [Fix] Add default scaler
                         "feature_cols": [] # [Fix] Add default empty list, will be populated on fly
                     }
                     
                 else:
                     # Traditional Pickle Loading (Mock Logic)
                     with open(selected_ver['path'], "rb") as f:
                         loaded_model_data = pickle.load(f)

             except Exception as e:
                 st.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}") 
                 if 'qlib' in str(e):
                      st.error("ğŸ’¡ Qlib ê´€ë ¨ ì—ëŸ¬ì…ë‹ˆë‹¤. ìˆœìˆ˜ PyTorch ëª¨ë“œ(.pth)ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.")

    if loaded_model_data:
        saved_ts = loaded_model_data.get('timestamp', 'Unknown')
        
        # [UX Improvement] Pre-Inference Model Specs
        st.info("â„¹ï¸ ì„ íƒëœ ëª¨ë¸ ìƒì„¸ ì •ë³´ (Model Specs)")
        spec_col1, spec_col2 = st.columns(2)
        with spec_col1:
            st.write(f"**ğŸ”¹ ëª¨ë¸ íƒ€ì…**: {loaded_model_data.get('model_type')}")
            st.write(f"**ğŸ”¹ ì˜ˆì¸¡ ê¸°ê°„ (Horizon)**: {loaded_model_data.get('horizon')}")
            st.write(f"**ğŸ”¹ í•™ìŠµ ê¸°ê°„**: {loaded_model_data.get('train_period', 'Unknown')}")
        
        with spec_col2:
            feat_lvl = loaded_model_data.get('feature_level', 'Unknown')
            feat_cnt = len(loaded_model_data.get('feature_cols', []))
            st.write(f"**ğŸ”¹ Feature ë³µì¡ë„**: {feat_lvl} ({feat_cnt} features)")
            univ_size = len(loaded_model_data.get('valid_tickers', []))
            st.write(f"**ğŸ”¹ í•™ìŠµ ìœ ë‹ˆë²„ìŠ¤ í¬ê¸°**: {univ_size}ê°œ ì¢…ëª©")
            st.write(f"**ğŸ”¹ ì €ì¥ ì¼ì‹œ**: {saved_ts}")
            
        st.divider()

        # [UX Improvement] Add Top-K slider specific for Inference here
        st.write("#### âš™ï¸ ì¶”ë¡  ì„¤ì • (Inference Settings)")
        top_k_inference = st.slider("ì¶”ì²œí•  ì¢…ëª© ìˆ˜ (Top K)", min_value=1, max_value=50, value=10, key="top_k_inf")
        
        # [Fix] Robust feature level retrieval
        feat_level = loaded_model_data.get('feature_level', 'Standard')
        
        # Action Buttons (Inference & Delete)
        col_act1, col_act2 = st.columns([3, 1])
        
        with col_act2:
             if st.button("ğŸ—‘ï¸ ëª¨ë¸ ì‚­ì œ (Delete)", type="primary"):
                 try:
                     os.remove(selected_ver['path'])
                     st.toast("âœ… ëª¨ë¸ íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                     st.rerun()
                 except Exception as e:
                     st.error(f"ì‚­ì œ ì‹¤íŒ¨: {e}")

        with col_act1:
            run_inference = st.button("âš¡ ì„ íƒëœ ëª¨ë¸ë¡œ ë°”ë¡œ ë¶„ì„ (Fast Inference)", type="primary")

        if run_inference:
            # Inject Backtest Data for Analysis Tab
            if 'backtest_data' in loaded_model_data:
                 st.session_state.trained_models[model_type] = loaded_model_data
                 
                 # [Fix] Render Saved Backtest Results Immediately
                 bd = loaded_model_data['backtest_data']
                 if bd and 'perf_df' in bd and not bd['perf_df'].empty:
                     metrics = bd.get('metrics', {})
                     res_df = bd['perf_df']
                     
                     st.markdown("### ğŸ“Š ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì˜ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼")
                     c1, c2, c3 = st.columns(3)
                     c1.metric("AI í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥ ", metrics.get("Total Return", "N/A"))
                     c2.metric("S&P 500 ìˆ˜ìµë¥ ", metrics.get("SPY Return", "N/A"))
                     c3.metric("ë™ì¼ ë¹„ì¤‘ (Equal)", metrics.get("EQ Return", "N/A"))
                     
                     st.line_chart(res_df)
                 else:
                     st.warning("âš ï¸ ì €ì¥ëœ ë°±í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                 
                 # [Supported Request] Display Model Characteristics
                 st.info("â„¹ï¸ ëª¨ë¸ ìƒì„¸ ìŠ¤í™ (Model Specifications)")
                 
                 spec_col1, spec_col2 = st.columns(2)
                 with spec_col1:
                     st.write(f"**ğŸ”¹ ëª¨ë¸ íƒ€ì…**: {loaded_model_data.get('model_type')}")
                     st.write(f"**ğŸ”¹ ì˜ˆì¸¡ ê¸°ê°„ (Horizon)**: {loaded_model_data.get('horizon')}")
                     st.write(f"**ğŸ”¹ í•™ìŠµ ê¸°ê°„**: {loaded_model_data.get('train_period', 'Unknown (Old Version)')}")
                 
                 with spec_col2:
                     feat_lvl = loaded_model_data.get('feature_level', 'Unknown')
                     feat_cnt = len(loaded_model_data.get('feature_cols', []))
                     st.write(f"**ğŸ”¹ Feature ë³µì¡ë„**: {feat_lvl} ({feat_cnt} features)")
                     
                     univ_size = len(loaded_model_data.get('valid_tickers', []))
                     st.write(f"**ğŸ”¹ í•™ìŠµ ìœ ë‹ˆë²„ìŠ¤ í¬ê¸°**: {univ_size}ê°œ ì¢…ëª©")
                     st.write(f"**ğŸ”¹ ì €ì¥ ì¼ì‹œ**: {saved_ts}")
                 st.divider()

            status_text = st.empty()
            progress_bar = st.progress(0)
            status_text.text("ìµœì‹  ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘ (Fast Mode - Last 200 Days)...")
            
            # Load Params
            model = loaded_model_data['model']
            scaler = loaded_model_data['scaler']
            feature_cols = loaded_model_data['feature_cols']
            saved_tickers = loaded_model_data.get('valid_tickers', [])
            
            # Use current tickers logic? Or saved? 
            # User wants to run on CURRENT universe but with SAVED model?
            # Generally, model trained on Tickers A,B,C might not work well on D,E,F if features are generic enough?
            # AI models are trained on patterns. Features (MA, RSI) are generic.
            # So we can apply saved model to NEW universe or CURRENT universe selection.
            # Let's use the CURRENT UI selection 'tickers' to be flexible.
            target_tickers = tickers if tickers else saved_tickers
            
            fast_data = {}
            fast_valid_tickers = []
            
            # Fast Download (Short period)
            end_date = pd.to_datetime("today")
            # Feature calculation needs ~120 days buffer
            start_date_fast = end_date - pd.Timedelta(days=365) # 1 year safe buffer
            
            for i, ticker in enumerate(target_tickers):
                try:
                    df = yf.download(ticker, start=start_date_fast, end=end_date, progress=False)
                    if isinstance(df.columns, pd.MultiIndex): df.columns = df.columns.get_level_values(0)
                    if 'Adj Close' not in df.columns:
                        if 'Close' in df.columns: df['Adj Close'] = df['Close']
                        else: continue
                    
                    df = df[['Open', 'High', 'Low', 'Adj Close', 'Volume']].copy()
                    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']
                    
                    # [Auto-Detect] Alpha158 Requirement
                    target_level = loaded_model_data.get('feature_level', 'Standard')
                    expected_dim = getattr(model, 'd_feat', None)
                    if expected_dim == 158:
                         target_level = "Alpha158"
                         if i==0: st.toast("ğŸ§ª Alpha158 Features Auto-Detected & Applied", icon="âš¡")
                    
                    # Feature Engineer
                    df, _ = calculate_feature_set(df, target_level)
                    
                    # [Fix] Auto-detect feature columns if missing (for PyTorch models)
                    if not feature_cols:
                         # For Alpha158, the function returns cols.
                         # If calculate_feature_set returned cols, use them.
                         # But wait, calculate_feature_set returns (df, cols).
                         # We captured it in _. Wait, code above says: df, _ = ...
                         # We MUST capture feature_cols!
                         pass # handled below
                         
                    # Re-retrieve feature_cols from calculation if needed
                    # Note: calculate_feature_set returns (df, feature_cols)
                    # We should use that value.
                    
                    # Let's fix the call signature interaction
                    df, computed_cols = calculate_feature_set(df, target_level)
                    if not feature_cols:
                        feature_cols = computed_cols
                        loaded_model_data['feature_cols'] = feature_cols
                    else:
                        # Validation: if dimension differs, prefer computed
                        if len(feature_cols) != len(computed_cols) and len(computed_cols) == 158:
                             feature_cols = computed_cols
                    
                    
                    # Drop NaN
                    df.dropna(inplace=True)
                    
                    if not df.empty:
                        fast_data[ticker] = df
                        fast_valid_tickers.append(ticker)
                        
                except Exception as e:
                    # [Debug] Show error if prediction fails
                    if i < 3: # Show only first few errors to avoid spam
                         st.error(f"Error for {ticker}: {e}")
                    pass
                progress_bar.progress((i+1)/len(target_tickers))
            
            status_text.text("AI ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
            
            # Prepare Session State for Results (Mocking the 'trained_models' state for the result viewer)
            # But wait, result viewer expects full_data, etc. 
            # We should populate session_state exactly as if we trained.
            
            st.session_state.trained_models[model_type] = {
                "model": model,
                "scaler": scaler,
                "feature_cols": feature_cols,
                "full_data": fast_data, # Only recent data
                "valid_tickers": fast_valid_tickers,
                "top_k": top_k_inference, # [Fix] Use the inference specific slider
                "feature_level": loaded_model_data['feature_level'],
                "horizon": horizon_option
            }
            
            st.success(f"âš¡ ë¹ ë¥¸ ë¶„ì„ ì™„ë£Œ! í•˜ë‹¨ 'ì˜¤ëŠ˜ì˜ ì¶”ì²œ PICK'ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            status_text.empty()
            
            # -----------------------------------------------------------------------------
            # [Fix] Result Rendering for Fast Inference (Same logic as Training)
            # -----------------------------------------------------------------------------
            st.divider()
            st.subheader("ğŸš€ ì˜¤ëŠ˜ì˜ ì¶”ì²œ PICK (Latest Predictions)")
            
            # A. Generate Predictions for the Latest Date
            recommendations = []
            
            st.write(f"ğŸ” Valid Tickers for Inference: {len(fast_valid_tickers)}") # Debug
            
            for ticker in fast_valid_tickers:
                df = fast_data[ticker]
                if df.empty: continue
                
                # Get Last Row
                last_row = df.iloc[[-1]] # Keep DataFrame format
                
                # Prepare Features
                try:
                    feat_vals = last_row[feature_cols].copy() # Ensure DataFrame/Series
                    
                    # 1. Scale (if scaler exists)
                    if scaler:
                         feat_scaled = scaler.transform(feat_vals.values)
                    else:
                         feat_scaled = feat_vals.values
                    
                    # 2. [Critical] Check Feature Dimension (Padding for Alpha158 vs Standard mismatch)
                    expected_dim = getattr(model, 'd_feat', None)
                    if expected_dim and feat_scaled.shape[1] < expected_dim:
                         current_dim = feat_scaled.shape[1]
                         pad_size = expected_dim - current_dim
                         import numpy as np
                         padding = np.zeros((feat_scaled.shape[0], pad_size))
                         feat_scaled = np.hstack([feat_scaled, padding])
                    
                    # Debug: Show shape and first few values
                    # if ticker == "AAPL":
                    #      st.write(f"AAPL Shape: {feat_scaled.shape}")
                    #      st.write(f"AAPL Feats: {feat_scaled[0][:5]}...")
                    
                    # Predict
                    score = 0
                    if isinstance(model, dict): # Ensemble
                         if "Linear" in model: 
                             p = model["Linear"].predict(feat_scaled)
                             score += p.item() if hasattr(p, 'item') else p[0]
                         if "LightGBM" in model: 
                             p = model["LightGBM"].predict(feat_scaled)
                             score += p.item() if hasattr(p, 'item') else p[0]
                         if "SVM" in model: 
                             p = model["SVM"].predict(feat_scaled)
                             score += p.item() if hasattr(p, 'item') else p[0]
                         score /= 3.0
                    else:
                        pred_res = model.predict(feat_scaled)
                        # Robust scalar extraction
                        if hasattr(pred_res, 'item'):
                            score = pred_res.item()
                        elif hasattr(pred_res, '__iter__') and len(pred_res) > 0:
                            score = pred_res.flat[0] # Handle any shape
                        else:
                            score = pred_res # Assume simple float
                    
                    # Debug Score
                    st.write(f"{ticker} Score: {score} (Type: {type(score)})")
                        
                    # Interpret Score
                    signal = "Hold (ê´€ë§)"
                    if score > 0.01: signal = "Strong Buy (ê°•ë ¥ ë§¤ìˆ˜) ğŸš€"
                    elif score > 0.005: signal = "Buy (ë§¤ìˆ˜) ğŸ“ˆ"
                    elif score < -0.01: signal = "Strong Sell (ê°•ë ¥ ë§¤ë„) ğŸ“‰"
                    elif score < -0.005: signal = "Sell (ë§¤ë„) ğŸ”»"
                    
                    recommendations.append({
                        "ì¢…ëª©ì½”ë“œ": ticker,
                        "ğŸš¦ ë§¤ë§¤ ì‹ í˜¸": signal,
                        "ğŸ“ˆ ì˜ˆìƒ ë“±ë½ë¥ ": f"{score:.2%}", # Format as %
                        "Raw_Score": score, # Hidden for sorting
                        "í˜„ì¬ê°€": f"{last_row['Close'].values[0]:,.0f}",
                        "ê¸°ì¤€ì¼": last_row.index[-1].strftime('%Y-%m-%d')
                    })
                except Exception as e:
                    # st.error(f"Inference Logic Error for {ticker}: {e}")
                    pass
            
            # Save Results to Session State for Display
            if recommendations:
                st.session_state.trained_models[model_type]['cached_recommendations'] = recommendations
                st.toast(f"âœ… Analysis Complete! Found {len(recommendations)} stocks.", icon="ğŸ‰")
            else:
                 st.warning("No recommendations generated.")

            pass

    elif 'scan_results' in st.session_state and not st.session_state.scan_results:
         st.info("í˜„ì¬ ê¸°ì¤€ íŠ¹ì´ íŒ¨í„´(ê³¨ë“ í¬ë¡œìŠ¤, ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„ ë“±)ì´ ë°œê²¬ëœ ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")


    # -----------------------------------------------------------------------------
    # Unified Result Rendering (Tabs) - Runs for both Training & Inference
    # -----------------------------------------------------------------------------
    if model_type in st.session_state.trained_models:
        st.divider()
        st.subheader("ğŸ“Š AI ëª¨ë¸ ë¶„ì„ ê²°ê³¼")
        
        # Load Data
        model_info = st.session_state.trained_models[model_type]
        
        tab1, tab2, tab3 = st.tabs(["ğŸ“ˆ ì„±ê³¼ ë¶„ì„ (Analysis)", "ğŸ“‹ ì˜¤ëŠ˜ì˜ ì¶”ì²œ (Recommendations)", "ğŸ“œ íˆìŠ¤í† ë¦¬ (History)"])
        
        # TAB 1: Analysis (Backtest)
        with tab1:
            st.markdown("### ğŸ“ˆ ë°±í…ŒìŠ¤íŒ… ì„±ê³¼ (Backtest Performance)")
            
            # Check for Backtest Data
            bd = model_info.get('backtest_data', {})
            if bd and 'perf_df' in bd and not bd['perf_df'].empty:
                 metrics = bd.get('metrics', {})
                 res_df = bd['perf_df']
                 
                 c1, c2, c3 = st.columns(3)
                 c1.metric("AI í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥ ", metrics.get("Total Return", "N/A"))
                 c2.metric("S&P 500 ìˆ˜ìµë¥ ", metrics.get("SPY Return", "N/A"))
                 c3.metric("ë™ì¼ ë¹„ì¤‘ (Equal)", metrics.get("EQ Return", "N/A"))
                 
                 st.line_chart(res_df)
            else:
                 st.info("ë°±í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (Short-term Inference Mode ë“±).")

        # TAB 2: Recommendations (Top-K)
        with tab2:
            st.markdown("### ğŸ“‹ ì˜¤ëŠ˜ì˜ Top Picks")
            
            # Priority: Use Cached Recommendations from Fast Inference
            if 'cached_recommendations' in model_info and model_info['cached_recommendations']:
                 recs = model_info['cached_recommendations']
                 # Convert to DataFrame
                 rec_df = pd.DataFrame(recs)
            else:
                 # Fallback: Training-time Inference (Original Logic)
                 current_model = model_info['model']
                 # ... (Old Loop Logic would go here if we wanted fallback, but for now we rely on cache)
                 rec_df = pd.DataFrame() # Empty
            
            if not rec_df.empty:
                 current_top_k = model_info.get('top_k', 5)
                 # Sort and Limit (Use Raw_Score for sorting)
                 final_picks = rec_df.sort_values(by="Raw_Score", ascending=False).head(current_top_k).copy()
                 final_picks = final_picks.reset_index(drop=True)
                 final_picks.index += 1 # 1-based index
                 
                 st.markdown(f"### ğŸš€ ì˜¤ëŠ˜ì˜ Top-{len(final_picks)} ì¶”ì²œ ì¢…ëª©")
                 
                 # Display Friendly DataFrame
                 display_cols = ["ì¢…ëª©ì½”ë“œ", "ğŸš¦ ë§¤ë§¤ ì‹ í˜¸", "ğŸ“ˆ ì˜ˆìƒ ë“±ë½ë¥ ", "í˜„ì¬ê°€", "ê¸°ì¤€ì¼"]
                 st.dataframe(final_picks[display_cols], use_container_width=True)
                 
                 st.caption("â„¹ï¸ **ì˜ˆìƒ ë“±ë½ë¥ **: AI ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë‹¤ìŒ ë³´ìœ  ê¸°ê°„(Horizon) ë™ì•ˆì˜ ìˆ˜ìµë¥ ì…ë‹ˆë‹¤.")
                 st.caption("â„¹ï¸ **ë§¤ë§¤ ì‹ í˜¸**: ì˜ˆìƒ ë“±ë½ë¥ ì´ 1% ì´ìƒì´ë©´ 'ê°•ë ¥ ë§¤ìˆ˜', 0.5% ì´ìƒì´ë©´ 'ë§¤ìˆ˜'ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.")
                 
                 # Save Portfolio Button
                 if st.button("ğŸ’¾ ì´ í¬íŠ¸í´ë¦¬ì˜¤ ì €ì¥í•˜ê¸° (Save History)"):
                     from datetime import datetime
                     save_entry = {
                        "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "model_type": model_type,
                        "holdings": final_picks.to_dict(orient='records'),
                        "top_k": len(final_picks),
                        "feature_level": model_info.get('feature_level', 'Unknown'),
                        "horizon": model_info.get('horizon', 'Unknown')
                     }
                     # Load existing & Append
                     hist = load_portfolio_history()
                     if not isinstance(hist, list): hist = []
                     hist.append(save_entry)
                     save_portfolio_history(hist)
                     st.success("í¬íŠ¸í´ë¦¬ì˜¤ê°€ ì´ë ¥ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                 st.info("ì•„ì§ ì¶”ì²œ ê²°ê³¼ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ 'Fast Inference' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
                
        # TAB 3: History
        with tab3:
            st.markdown("### ğŸ“œ ë‚˜ì˜ í¬íŠ¸í´ë¦¬ì˜¤ ì €ì¥ ì´ë ¥")
            
            # [Restored Feature] Import History
            with st.expander("ğŸ“‚ ë°±ì—… íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸° (Import History)", expanded=False):
                st.caption("ê¸°ì¡´ì— ë‹¤ìš´ë¡œë“œ ë°›ì•˜ë˜ `portfolio_history.json` íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë³µì›ë©ë‹ˆë‹¤.")
                uploaded_file = st.file_uploader("JSON íŒŒì¼ ì„ íƒ", type=["json"], key="history_uploader")
                
                if uploaded_file is not None:
                    try:
                        imported_data = json.load(uploaded_file)
                        if isinstance(imported_data, list):
                            # Append to current history
                            current_hist = load_portfolio_history()
                            if not isinstance(current_hist, list): current_hist = []
                            
                            # Deduplication check (simple date check)
                            existing_dates = {x.get('date') for x in current_hist}
                            count = 0
                            for item in imported_data:
                                if item.get('date') not in existing_dates:
                                    current_hist.append(item)
                                    count += 1
                                    
                            if count > 0:
                                save_portfolio_history(current_hist)
                                st.success(f"âœ… {count}ê°œì˜ í¬íŠ¸í´ë¦¬ì˜¤ ì´ë ¥ì´ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤. í™”ë©´ì„ ìƒˆë¡œê³ ì¹¨í•©ë‹ˆë‹¤.")
                                st.rerun()
                            else:
                                st.warning("ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì´ë ¥ë“¤ì…ë‹ˆë‹¤.")
                        else:
                            st.error("ì˜¬ë°”ë¥´ì§€ ì•Šì€ JSON í˜•ì‹ì…ë‹ˆë‹¤.")
                    except Exception as e:
                        st.error(f"íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            
            # Load History
            hist_data = load_portfolio_history()
            
            if not hist_data:
                st.info("ì•„ì§ ì €ì¥ëœ í¬íŠ¸í´ë¦¬ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                # History Download Button
                json_str = json.dumps(hist_data, default=str, indent=4)
                st.download_button(
                    label="ğŸ’¾ ì „ì²´ ì´ë ¥ ë‹¤ìš´ë¡œë“œ (Backup)",
                    data=json_str,
                    file_name=f"portfolio_history_{datetime.now().strftime('%Y%m%d')}.json",
                    mime="application/json"
                )
                st.divider()

                # Show list
                # Reverse order
                if isinstance(hist_data, list):
                    for idx, item in enumerate(reversed(hist_data)):
                        idx_real = len(hist_data) - 1 - idx
                        with st.expander(f"ğŸ“… {item.get('date', 'Unknown')} - {item.get('model_type')} ({len(item.get('holdings', []))} Stocks)"):
                            c_del, c_view = st.columns([1, 5])
                            with c_del:
                                if st.button("ğŸ—‘ï¸ ì‚­ì œ", key=f"del_hist_{idx_real}"):
                                    del hist_data[idx_real]
                                    save_portfolio_history(hist_data)
                                    st.rerun()
                                    
                            st.write(f"**Top-K**: {item.get('top_k')} | **Horizon**: {item.get('horizon')}")
                            h_df = pd.DataFrame(item.get('holdings', []))
                            st.dataframe(h_df)
                else:
                    st.error("ë°ì´í„° í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    



# -----------------------------------------------------------------------------
# ğŸ” ETF êµ¬ì„± ì¢…ëª© ê²€ìƒ‰ (Reverse Search)
# -----------------------------------------------------------------------------
elif selection == "ğŸ” ETF êµ¬ì„± ì¢…ëª© ê²€ìƒ‰":
    st.title("ğŸ” ETF êµ¬ì„± ì¢…ëª© ê²€ìƒ‰ (Reverse Search)")
    st.caption("íŠ¹ì • ì¢…ëª©ì„ ë‹´ê³  ìˆëŠ” ETFë¥¼ ê²€ìƒ‰í•˜ê³ , ë¹„ì¤‘ ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤. (KRX ì‹¤ì‹œê°„ ë°ì´í„° ê¸°ë°˜)")

    import FinanceDataReader as fdr

    # 1. ìœ íš¨í•œ ë°ì´í„°ê°€ ìˆëŠ” ìµœì‹  ì˜ì—…ì¼ êµ¬í•˜ê¸°
    # (ì£¼ì˜: fdrì€ ë³„ë„ ë‚ ì§œ ì²´í¬ ì—†ì´ ìµœì‹  ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ ì˜¤ëŠ˜ ë‚ ì§œ ë˜ëŠ” ì•ˆì „í•œ í‰ì¼ì„ ë°˜í™˜)
    @st.cache_data(ttl=3600*12) 
    def get_latest_biz_date():
        # ETF PDF ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ë•ŒëŠ” ë‚ ì§œê°€ ì¤‘ìš”í•˜ë¯€ë¡œ, í‰ì¼ì¸ì§€ ì²´í¬
        curr = datetime.now()
        # ë§Œì•½ ì£¼ë§ì´ë©´ ê¸ˆìš”ì¼ë¡œ ì´ë™
        while curr.weekday() > 4:
            curr -= timedelta(days=1)
        return curr.strftime("%Y%m%d")

    target_date = get_latest_biz_date()
    st.info(f"ğŸ“… ë°ì´í„° ê¸°ì¤€ì¼: **{target_date[:4]}-{target_date[4:6]}-{target_date[6:]}** (KRX)")

    # 2. ë°ì´í„° ìˆ˜ì§‘ ë° ìºì‹±
    @st.cache_data(ttl=3600*24, show_spinner=False) # 24ì‹œê°„ ìºì‹œ
    def get_all_etf_data(date):
        """
        ëª¨ë“  ETFì˜ êµ¬ì„± ì¢…ëª©(PDF) ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ Dictionary í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        Key: Ticker, Value: Data (Name, PDF_DataFrame)
        """
        # A. ETF ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (pykrx ëŒ€ì‹  fdr ì‚¬ìš© - ì¸ì½”ë”© ì´ìŠˆ ìš°íšŒ)
        tickers = []
        try:
            # KRX ETF ë¦¬ìŠ¤íŠ¸ (Symbol, Name ë“± í¬í•¨)
            etf_list_df = fdr.StockListing('ETF/KR')
            tickers = etf_list_df['Symbol'].tolist()
            
            # [í…ŒìŠ¤íŠ¸ ëª¨ë“œ] ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ìƒìœ„ 20ê°œë§Œ ìš°ì„  í…ŒìŠ¤íŠ¸
            # í…ŒìŠ¤íŠ¸ ì™„ë£Œ í›„ ì•„ë˜ ë‘ ì¤„ ì£¼ì„ ì²˜ë¦¬ ë˜ëŠ” ì‚­ì œí•˜ë©´ ì „ì²´ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥
            if len(tickers) > 20: 
                 tickers = tickers[:20] 
                 st.info(f"âš¡ [í…ŒìŠ¤íŠ¸ ëª¨ë“œ] ë¹ ë¥¸ í™•ì¸ì„ ìœ„í•´ ì „ì²´ {len(etf_list_df)}ê°œ ì¤‘ **ìƒìœ„ 20ê°œ ETF**ë§Œ ìŠ¤ìº”í•©ë‹ˆë‹¤.")
        except Exception as e:
            # st.error(f"ETF ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ (FDR): {e}")
            pass
        
        # Fallback: ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ì‹œ ì£¼ìš” ETF í•˜ë“œì½”ë”©
        if not tickers:
            tickers = [
                "069500", # KODEX 200
                "371460", # TIGER ì°¨ì´ë‚˜ì „ê¸°ì°¨SOLACTIVE
                "122630", # KODEX ë ˆë²„ë¦¬ì§€
                "252670", # KODEX 200ì„ ë¬¼ì¸ë²„ìŠ¤2X
                "233740", # KODEX ì½”ìŠ¤ë‹¥150ë ˆë²„ë¦¬ì§€
                "251340", # KODEX ì½”ìŠ¤ë‹¥150ì„ ë¬¼ì¸ë²„ìŠ¤
                "102110", # TIGER 200
                "278530", # KODEX 200TR
                "278540", # TIGER 200TR
                "360750", # TIGER ë¯¸êµ­S&P500
                "360200", # TIGER ë¯¸êµ­ë‚˜ìŠ¤ë‹¥100
            ]
            st.warning("âš ï¸ ETF ì „ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•´ ì£¼ìš” 11ê°œ ETFë§Œ ìŠ¤ìº”í•©ë‹ˆë‹¤.")
            # st.success(f"ì´ {len(tickers)}ê°œì˜ ETF ë¦¬ìŠ¤íŠ¸ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
            pass
            
        etf_data = {}
        error_count = 0


        
        # ì§„í–‰ë¥  í‘œì‹œ (ìµœì´ˆ ì‹¤í–‰ ì‹œì—ë§Œ ë³´ì„)
        progress_text = "KRXì—ì„œ ëª¨ë“  ETF ë°ì´í„°(PDF)ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤... (ìµœì´ˆ 1íšŒ ì‹¤í–‰ ì‹œ 3~5ë¶„ ì†Œìš”)"
        my_bar = st.progress(0, text=progress_text)
        
        total = len(tickers)
        
        # FDRì—ì„œ ê°€ì ¸ì˜¨ ì´ë¦„ ë§¤í•‘ (Name Column í™•ì¸ í•„ìš”, ë³´í†µ 'Name')
        name_map = {}
        if 'Name' in etf_list_df.columns:
            name_map = etf_list_df.set_index('Symbol')['Name'].to_dict()
        
        total = len(tickers)
        
        
        last_error = None
        
        for i, ticker in enumerate(tickers):
            pdf = None
            try:
                # 1. pykrx ì‹œë„
                try:
                    pdf = stock.get_etf_portfolio_deposit_file(ticker, date)
                except:
                    pdf = None

                # 2. ì‹¤íŒ¨ ì‹œ: Daum Finance API (Kakao Pay) - ì°¨ë‹¨ ê°€ëŠ¥ì„± ë‚®ìŒ
                if pdf is None or pdf.empty:
                    try:
                        # Daum URL: https://finance.daum.net/api/etf/constituents?symbolCode=A069500
                        # Tickerì— 'A' ë¶™ì—¬ì•¼ í•¨
                        daum_ticker = f"A{ticker}"
                        url = f"https://finance.daum.net/api/etf/constituents?symbolCode={daum_ticker}"
                        
                        headers = {
                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                            'Referer': f'https://finance.daum.net/quotes/{daum_ticker}',
                            'Accept': 'application/json, text/plain, */*',
                            'Host': 'finance.daum.net'
                        }
                        
                        resp = requests.get(url, headers=headers, verify=False, timeout=5)
                        
                        if resp.status_code == 200:
                            data_json = resp.json()
                            if "data" in data_json:
                                holdings = data_json["data"]
                                temp_df = pd.DataFrame(holdings)
                                
                                if not temp_df.empty:
                                    # ì»¬ëŸ¼ ë§¤í•‘ needed
                                    # Daum fields: symbolCode, name, tradePrice, weight
                                    rename_map = {
                                        'name': 'Name',
                                        'weight': 'ë¹„ì¤‘',
                                        'tradePrice': 'ê¸ˆì•¡', # ì •í™•íˆëŠ” í˜„ì¬ê°€ì§€ë§Œ ê¸ˆì•¡ ëŒ€ìš©ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸. 
                                        # í•˜ì§€ë§Œ ETF PDFì˜ 'ê¸ˆì•¡'ì€ 'ë³´ìœ ê¸ˆì•¡'ì´ë¯€ë¡œ tradePrice(í˜„ì¬ê°€)ì™€ ë‹¤ë¦„.
                                        # ë¹„ì¤‘ì´ í•µì‹¬.
                                        'symbolCode': 'Code'
                                    }
                                    pdf = temp_df.rename(columns=rename_map)
                                    # Codeì—ì„œ 'A' ì œê±° (A005930 -> 005930)
                                    if 'Code' in pdf.columns:
                                        pdf['Code'] = pdf['Code'].str.replace('A', '', regex=False)
                                    
                                    # ë¹„ì¤‘ì´ 0~100 ì‚¬ì´ ìˆ«ìì¸ì§€ í™•ì¸. Daumì€ ë³´í†µ 0.25 (1% ë¯¸ë§Œ) or 25.0 ?
                                    # í™•ì¸ ê²°ê³¼ Daumì€ 0.15 (=0.15%) ì‹ìœ¼ë¡œ ì¤„ ìˆ˜ë„ ìˆê³  15.0ì¼ ìˆ˜ë„ ìˆìŒ. 
                                    # ì¼ë‹¨ ê·¸ëŒ€ë¡œ ë‘ .
                                    
                                    if 'ê¸ˆì•¡' not in pdf.columns:
                                         pdf['ê¸ˆì•¡'] = 0
                            
                    except Exception as e_daum:
                        if last_error is None:
                            last_error = f"Daum API Error: {str(e_daum)}"
                        pass

                # 3. ìµœí›„ì˜ ìˆ˜ë‹¨: Yahoo Finance (yfinance) - í•´ì™¸ IP(Streamlit Cloud)ì—ì„œ ì‘ë™ ê°€ëŠ¥
                if pdf is None or pdf.empty:
                    try:
                        # Ticker format: 069500.KS
                        yf_ticker = f"{ticker}.KS"
                        fund = yf.Ticker(yf_ticker)
                        
                        # Top Holdings ê°€ì ¸ì˜¤ê¸° (ë³´í†µ ìƒìœ„ 10ê°œë§Œ ì œê³µë¨)
                        holdings_df = None
                        try:
                            # funds_data.top_holdingsëŠ” pandas DF ë¦¬í„´ (Name, Symbol, Holding Percent ë“±)
                            # yfinance ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. ìµœì‹ ë²„ì „ ê¸°ì¤€ ì‹œë„.
                            # í˜¹ì€ info['holdings'] ë“±
                            # ì—¬ê¸°ì„  ì•ˆì „í•˜ê²Œ funds_data ì ‘ê·¼ ì‹œë„
                            if hasattr(fund, 'funds_data') and fund.funds_data:
                                holdings_df = fund.funds_data.top_holdings
                        except:
                            pass
                            
                        if holdings_df is not None and not holdings_df.empty:
                            # ì»¬ëŸ¼ ë§¤í•‘: Name, Symbol, Holding % (0.05 form or 5.0 form)
                            # yfinance returns: index=Symbol, columns=['Name', 'Holding %', 'Buying', 'Selling']
                            # Reset index to get Symbol as column
                            holdings_df = holdings_df.reset_index()
                            
                            rename_map = {
                                'Name': 'Name',
                                'Symbol': 'Code',
                                'Holding %': 'ë¹„ì¤‘' 
                            }
                            # ì»¬ëŸ¼ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ í™•ì¸
                            cols = holdings_df.columns
                            if 'Name' in cols and 'Holding %' in cols:
                                pdf = holdings_df.rename(columns=rename_map)
                                # ë¹„ì¤‘ì´ 0.xx í˜•íƒœë©´ * 100 í•´ì•¼í•¨ (yfinanceëŠ” ë³´í†µ 0.0524 í˜•íƒœë¡œ ì¤Œ)
                                # ê·¼ë° yf ìµœì‹ ì€ ì´ë¯¸ %ë‹¨ìœ„(5.24)ì¼ ìˆ˜ë„ ìˆìŒ. í™•ì¸ í•„ìš”. 
                                # ë³´í†µ funds_dataëŠ” 0~1 scaleì¸ ê²½ìš°ê°€ ë§ìŒ -> í™•ì¸ ë¶ˆê°€í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë‘ 
                                # ê¸ˆì•¡ ì •ë³´ ì—†ìŒ
                                pdf['ê¸ˆì•¡'] = 0
                                
                    except Exception as e_yf:
                        if last_error is None:
                             last_error = f"Yahoo Error: {str(e_yf)}"
                        pass
                
                # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
                if pdf is not None and not pdf.empty:
                    # FDR Name Map ì‚¬ìš©
                    name = name_map.get(str(ticker), str(ticker))
                    
                    etf_data[ticker] = {
                        "name": name,
                        "pdf": pdf 
                    }
            except Exception as e:
                error_count += 1
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ë„ˆë¬´ ìì£¼í•˜ë©´ ëŠë ¤ì§€ë¯€ë¡œ 5% ë‹¨ìœ„ or 10ê°œ ë‹¨ìœ„)
            if i % 10 == 0:
                my_bar.progress((i + 1) / total, text=f"{progress_text} ({i+1}/{total})")
                
        my_bar.empty()
        
        # Debug Info: ì²«ë²ˆì§¸ ì—ëŸ¬ ë³´ì—¬ì£¼ê¸° (ì‚¬ìš©ì í”¼ë“œë°±ìš©)
        if last_error and not etf_data:
             with st.expander("âš ï¸ ë°ì´í„° ìˆ˜ì§‘ ì—ëŸ¬ ìƒì„¸ (Debug Logs)"):
                st.write(f"Last Error: {last_error}")

        
        if error_count > 0:
            st.warning(f"{error_count}ê°œì˜ ETF ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ (ìƒì¥íì§€ ë“± ì´ìœ ).")
            
        return etf_data

    # ë°ì´í„° ë¡œë”© Trigger
    with st.spinner("ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë™ê¸°í™” ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
        all_etf_data = get_all_etf_data(target_date)

    # 3. ê²€ìƒ‰ UI
    st.divider()
    search_query = st.text_input("ê²€ìƒ‰í•  ì¢…ëª©ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì‚¼ì„±ì „ì, NAVER)", placeholder="ì¢…ëª©ëª… ì…ë ¥ í›„ Enter").strip()

    if search_query:
        # A. ê²€ìƒ‰ ë¡œì§
        found_etfs = []
        
        # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ê²Œ í‹°ì»¤ì¸ì§€ ì´ë¦„ì¸ì§€ ëª¨ë¦„ -> ì´ë¦„ìœ¼ë¡œ ë§¤ì¹­ ì‹œë„
        # pykrxì˜ PDF ë°ì´í„°ì—ëŠ” ì¢…ëª©ì½”ë“œê°€ ì¸ë±ìŠ¤ì´ê³ , ì¢…ëª©ëª…ì€ ì—†ì„ ìˆ˜ ìˆìŒ.
        # ë”°ë¼ì„œ "ì‚¼ì„±ì „ì"ë¥¼ "005930"ìœ¼ë¡œ ë³€í™˜í•˜ê±°ë‚˜, PDF ë‚´ì— ì¢…ëª©ëª…ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•¨.
        # get_etf_portfolio_deposit_file() ê²°ê³¼ëŠ” ë³´í†µ ì¸ë±ìŠ¤=í‹°ì»¤, ì»¬ëŸ¼=[ê³„ì•½ìˆ˜, ê¸ˆì•¡, ë¹„ì¤‘] í˜•íƒœì„. ì¢…ëª©ëª…ì´ ì—†ìŒ.
        # í•´ê²°ì±…:
        # 1. KOSPI/KOSDAQ ì „ ì¢…ëª© ë§ˆìŠ¤í„° ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ {ì´ë¦„: í‹°ì»¤} ë§¤í•‘ì„ ë§Œë“¦.
        # 2. ì‚¬ìš©ìê°€ ì…ë ¥í•œ "ì‚¼ì„±ì „ì" -> "005930" ë³€í™˜.
        # 3. ê° ETFì˜ PDF ì¸ë±ìŠ¤(í‹°ì»¤)ì— "005930"ì´ ìˆëŠ”ì§€ í™•ì¸.
        
        @st.cache_data
        def get_stock_name_map(date):
            # 1. FDR KRX ì „ì²´ ë¦¬ìŠ¤íŠ¸ ì‹œë„
            name_map = {}
            try:
                df_krx = fdr.StockListing('KRX')
                if not df_krx.empty:
                    name_map = df_krx.set_index('Name')['Symbol'].to_dict()
            except Exception as e:
                pass
            
            # 2. ì‹¤íŒ¨í•˜ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ KOSPI/KOSDAQ ê°œë³„ ì‹œë„ (Fallback)
            if not name_map:
                try:
                    df_kospi = fdr.StockListing('KOSPI')
                    df_kosdaq = fdr.StockListing('KOSDAQ')
                    if not df_kospi.empty:
                        name_map.update(df_kospi.set_index('Name')['Symbol'].to_dict())
                    if not df_kosdaq.empty:
                        name_map.update(df_kosdaq.set_index('Name')['Symbol'].to_dict())
                except:
                    pass
            # 3. ìµœí›„ì˜ ìˆ˜ë‹¨: ì£¼ìš” ì¢…ëª© í•˜ë“œì½”ë”© (ë„¤íŠ¸ì›Œí¬/íŒŒì‹± ì „ë©´ ì‹¤íŒ¨ ì‹œ ëŒ€ë¹„)
            if not name_map:
                name_map = {
                    "ì‚¼ì„±ì „ì": "005930",
                    "SKí•˜ì´ë‹‰ìŠ¤": "000660",
                    "NAVER": "035420",
                    "ì¹´ì¹´ì˜¤": "035720",
                    "LGì—ë„ˆì§€ì†”ë£¨ì…˜": "373220",
                    "í˜„ëŒ€ì°¨": "005380",
                    "POSCOí™€ë”©ìŠ¤": "005490",
                    "ê¸°ì•„": "000270",
                    "KBê¸ˆìœµ": "105560"
                }
            
            return name_map

        name_map = get_stock_name_map(target_date)
        
        # Debug Info: í™œì„±í™”í•´ì„œ ìƒíƒœ í™•ì¸
        st.warning(f"ğŸ” Debug Info: Loaded {len(name_map)} stocks. " 
                   f"Sample: {list(name_map.keys())[:5] if name_map else 'Empty'}")

        
        # ê²€ìƒ‰ì–´ ë§¤ì¹­ (ì •í™•ì¹˜ & í¬í•¨)
        target_ticker = name_map.get(search_query) # ì •í™•íˆ ì¼ì¹˜
        
        # ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ í¬í•¨ ê²€ìƒ‰ (ì²« ë²ˆì§¸ ë°œê²¬ëœ ê²ƒ)
        if not target_ticker:
            candidates = [name for name in name_map.keys() if search_query.upper() in name.upper()]
            if len(candidates) > 0:
                # ì„ íƒì§€ ì œê³µ? ì•„ë‹ˆë©´ ì²«ë²ˆì§¸?
                # UXìƒ ëª¨í˜¸í•˜ë©´ ê°€ì¥ ìœ ì‚¬í•œ ê²ƒ ì„ íƒ or Selectbox
                if len(candidates) == 1:
                    target_ticker = name_map[candidates[0]]
                    st.success(f"'{candidates[0]}' ({target_ticker}) ì¢…ëª©ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
                else:
                    st.info(f"ê²€ìƒ‰ì–´ '{search_query}'ì™€ ìœ ì‚¬í•œ ì¢…ëª©: {', '.join(candidates[:5])} ...")
                    selected_name = st.selectbox("ì¢…ëª©ì„ ì„ íƒí•˜ì„¸ìš”:", candidates)
                    target_ticker = name_map[selected_name]
            else:
                st.error("í•´ë‹¹í•˜ëŠ” ì¢…ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()
        
        # B. ETF í•„í„°ë§
        result_list = []
        
        # Debug: ë°ì´í„°ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if not all_etf_data:
            st.error("ETF ë°ì´í„°ë¥¼ í•˜ë‚˜ë„ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (KRX/ë„¤ì´ë²„ ì ‘ì† ì‹¤íŒ¨)")
        else:
            # st.info(f"Debug: {len(all_etf_data)}ê°œ ETF ë°ì´í„° ìŠ¤ìº” ì¤‘...")
            pass

        for etf_ticker, data in all_etf_data.items():
            pdf_df = data['pdf']
            found = False
            row = None
            
            # 1. Tickerë¡œ ê²€ìƒ‰ (pykrx ë°ì´í„°ì¸ ê²½ìš° Indexê°€ Ticker)
            if target_ticker in pdf_df.index:
                row = pdf_df.loc[target_ticker]
                found = True
            
            # 2. Tickerê°€ ì»¬ëŸ¼ì— ìˆëŠ”ì§€ í™•ì¸
            elif 'Code' in pdf_df.columns and target_ticker in pdf_df['Code'].values:
                # í•´ë‹¹ ë¡œìš° ì°¾ê¸°
                row = pdf_df[pdf_df['Code'] == target_ticker].iloc[0]
                found = True

            # 3. ì¢…ëª©ëª…ìœ¼ë¡œ ê²€ìƒ‰ (Naver í¬ë¡¤ë§ ë°ì´í„°ì¸ ê²½ìš° Tickerê°€ ì—†ì„ ìˆ˜ ìˆìŒ)
            if not found:
                # ë¬¸ìì—´ ì»¬ëŸ¼ë“¤ ì¤‘ì—ì„œ ì¢…ëª©ëª…ì´ í¬í•¨ëœ í–‰ ì°¾ê¸°
                # search_query: "ì‚¼ì„±ì „ì"
                for col in pdf_df.columns:
                    # ë°ì´í„° íƒ€ì…ì´ ë¬¸ìì—´ì´ê±°ë‚˜ objectì¸ ê²½ìš°
                    if pdf_df[col].dtype == object or pdf_df[col].dtype == str:
                        # ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜ í¬í•¨ë˜ëŠ”ì§€ í™•ì¸ (ì—¬ê¸°ì„  ì •í™• ì¼ì¹˜ ì„ í˜¸í•˜ë‚˜, ê³µë°± ì´ìŠˆ ë“±ìœ¼ë¡œ í¬í•¨ ì‚¬ìš©)
                        # í•˜ì§€ë§Œ "ì‚¼ì„±" ê²€ìƒ‰ ì‹œ "ì‚¼ì„±ì „ì"ê°€ ê±¸ë¦¬ëŠ”ê±´ ì˜ë„ëœ ë™ì‘.
                        # "ì‚¼ì„±ì „ì" ê²€ìƒ‰ ì‹œ "ì‚¼ì„±ì „ì" í–‰ì„ ì°¾ì•„ì•¼ í•¨.
                        
                        # ì•ˆì „í•œ ì²˜ë¦¬ë¥¼ ìœ„í•´ string ë³€í™˜ í›„ ê²€ìƒ‰
                        matches = pdf_df[pdf_df[col].astype(str).str.contains(search_query, na=False)]
                        if not matches.empty:
                            row = matches.iloc[0]
                            found = True
                            break
            
            if found and row is not None:
                # ì»¬ëŸ¼ëª…ì´ ì¡°ê¸ˆì”© ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¹„ì¤‘ ì»¬ëŸ¼ ì°¾ê¸°
                weight = 0
                
                # ë‹¤ì–‘í•œ ì»¬ëŸ¼ëª… ì‹œë„
                cols = pdf_df.columns
                weight_col = next((c for c in cols if 'ë¹„ì¤‘' in c), None) # 'ë¹„ì¤‘', 'ë¹„ì¤‘(%)', 'êµ¬ì„±ë¹„ì¤‘' ë“±
                amount_col = next((c for c in cols if 'ê¸ˆì•¡' in c or 'í‰ê°€ì•¡' in c), None) # 'ê¸ˆì•¡', 'í‰ê°€ê¸ˆì•¡'
                
                if weight_col:
                    weight = row[weight_col]
                elif amount_col: 
                    # ê¸ˆì•¡ë§Œ ìˆê³  ë¹„ì¤‘ ì—†ìœ¼ë©´ ì „ì²´ í•© ëŒ€ë¹„ ë¹„ìœ¨ ê³„ì‚°
                    # í•´ë‹¹ ì»¬ëŸ¼ì˜ í•©
                    try:
                        total_amt = pdf_df[amount_col].sum()
                        if total_amt > 0:
                            weight = (row[amount_col] / total_amt) * 100
                    except:
                        pass
                
                # ë¹„ì¤‘ì´ ë¬¸ìì—´ì¸ ê²½ìš° ì²˜ë¦¬ (Naver ë“±)
                if isinstance(weight, str):
                    try:
                        weight = float(weight.replace('%', '').strip())
                    except:
                        pass
                
                result_list.append({
                    "ETF ì½”ë“œ": etf_ticker,
                    "ETFëª…": data['name'],
                    "ì¢…ëª© ë¹„ì¤‘(%)": weight,
                    "ë³´ìœ  ê¸ˆì•¡": row[amount_col] if amount_col else 0
                })

        # C. ê²°ê³¼ ì¶œë ¥
        # C. ê²°ê³¼ ì¶œë ¥
        if result_list:
            df_result = pd.DataFrame(result_list)
            # ë¹„ì¤‘ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            df_result = df_result.sort_values(by="ì¢…ëª© ë¹„ì¤‘(%)", ascending=False).reset_index(drop=True)
            
            st.success(f"ì´ {len(df_result)}ê°œì˜ ETFê°€ í•´ë‹¹ ì¢…ëª©ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
            
            # í…Œì´ë¸”
            st.dataframe(
                df_result.style.format({"ì¢…ëª© ë¹„ì¤‘(%)": "{:.2f}", "ë³´ìœ  ê¸ˆì•¡": "{:,.0f}"}),
                use_container_width=True
            )
            
            # ì°¨íŠ¸ (ìƒìœ„ 5ê°œì¸ì§€, ì‚¬ìš©ì ì„ íƒì¸ì§€) -> ìƒìœ„ 10ê°œ ì‹œê°í™”
            top_n = df_result.head(10)
            fig = px.bar(
                top_n, 
                x="ETFëª…", 
                y="ì¢…ëª© ë¹„ì¤‘(%)", 
                title=f"'{search_query}' ë¹„ì¤‘ì´ ë†’ì€ ETF Top 10",
                color="ì¢…ëª© ë¹„ì¤‘(%)",
                text="ì¢…ëª© ë¹„ì¤‘(%)"
            )
            fig.update_traces(texttemplate='%{text:.2f}%', textposition='outside')
            st.plotly_chart(fig, use_container_width=True)
            st.warning("í•´ë‹¹ ì¢…ëª©ì„ í¬í•¨í•˜ëŠ” ETFê°€ ì—†ìŠµë‹ˆë‹¤.")

# -----------------------------------------------------------------------------
# 1. Helper Functions
# -----------------------------------------------------------------------------
# History functions moved to top

# -----------------------------------------------------------------------------
# ğŸ¤– ë¡œë³´ ì–´ë“œë°”ì´ì € (Demo) - React Port
# -----------------------------------------------------------------------------
def page_robo_advisor():
    # HTML ê³µë°± ì œê±° í—¬í¼ í•¨ìˆ˜ (ëª¨ë“  ë“¤ì—¬ì“°ê¸° ì œê±°í•˜ì—¬ Markdown ì½”ë“œë¸”ë¡ ì¸ì‹ ë°©ì§€)
    def clean_html(html_str):
        return "\n".join([line.strip() for line in html_str.splitlines() if line.strip()])

    # ëª¨ë°”ì¼ ë ˆì´ì•„ì›ƒì„ ìœ„í•œ CSS
    st.markdown(clean_html("""
        <style>
        /* ëª¨ë°”ì¼ í™”ë©´ ì‹œë®¬ë ˆì´ì…˜ ì»¨í…Œì´ë„ˆ */
        .mobile-container {
            max-width: 450px;
            margin: 0 auto;
            background-color: #F9FAFB; /* gray-50 */
            min-height: 100vh;
            padding: 20px;
            border-radius: 20px;
            box-shadow: 0 10px 25px rgba(0,0,0,0.1);
            font-family: 'Noto Sans KR', sans-serif;
            color: #1F2937; /* gray-800 */
        }
        .stTabs [data-baseweb="tab-list"] {
            justify-content: center;
        }
        /* ì¹´ë“œ ìŠ¤íƒ€ì¼ ìœ í‹¸ë¦¬í‹° */
        .card {
            background-color: white;
            border-radius: 12px;
            padding: 16px;
            box-shadow: 0 1px 2px rgba(0,0,0,0.05);
            border: 1px solid #F3F4F6;
            margin-bottom: 12px;
        }
        </style>
    """), unsafe_allow_html=True)

    # -----------------------------
    # 1. Mock Data Definition
    # -----------------------------
    ongoing_changes = [
        {"id": 1, "name": "KCGIìƒëŸ¬ë¦¬ë§¨ì¦ê¶Œìíˆ¬ìì‹ íƒ(ì£¼ì‹)", "type": "out", "before": 10.68, "after": 0, "diff": "-10.68%", "region": "ê¸€ë¡œë²Œ", "category": "ê¸€ë¡œë²Œì£¼ì‹", "tags": ["#ê¸€ë¡œë²Œê°€ì¹˜ì£¼", "#ì§€ë°°êµ¬ì¡°ê°œì„ ", "#ESGí…Œë§ˆ"]},
        {"id": 3, "name": "í•œí™”ì²œì—°ìì›ì¦ê¶Œìíˆ¬ìì‹ íƒ(ì£¼ì‹)", "type": "new", "before": 0, "after": 4.74, "diff": "+4.74%", "region": "ê¸€ë¡œë²Œ", "category": "ê¸€ë¡œë²Œì£¼ì‹", "tags": ["#ì—ë„ˆì§€/ê´‘ë¬¼", "#ì²œì—°ìì›ê¸°ì—…", "#ì‹¤ë¬¼ìì‚°íˆ¬ì"]},
        {"id": 2, "name": "í•˜ë‚˜PIMCOê¸€ë¡œë²Œì¸ì»´í˜¼í•©ìì‚°(ì±„ê¶Œ)", "type": "buy", "before": 2.52, "after": 8.72, "diff": "+6.20%", "region": "ê¸€ë¡œë²Œ", "category": "ì„ ì§„êµ­ì±„ê¶Œ1", "tags": ["#ê¸€ë¡œë²Œì±„ê¶Œ", "#ì›”ì§€ê¸‰ì‹", "#ì•ˆì •ì ìˆ˜ìµ"]},
        {"id": 4, "name": "êµë³´ì•…ì‚¬íŒŒì›Œì¸ë±ìŠ¤(ì£¼ì‹)", "type": "new", "before": 0, "after": 2.88, "diff": "+2.88%", "region": "í•œêµ­", "category": "êµ­ë‚´ì£¼ì‹", "tags": ["#KOSPI200", "#êµ­ë‚´ëŒ€í˜•ì£¼", "#ì§€ìˆ˜ì¶”ì¢…"]},
        {"id": 5, "name": "í‚¤ì›€ìŠˆë¡œë”ì´ë¨¸ì§•ìœ„ë„ˆìŠ¤(ì£¼ì‹í˜¼í•©)", "type": "sell", "before": 9.07, "after": 7.75, "diff": "-1.32%", "region": "ì‹ í¥êµ­", "category": "ì‹ í¥êµ­ì£¼ì‹", "tags": ["#ì‹ í¥êµ­ì„±ì¥ì£¼", "#ì•„ì‹œì•„/ë‚¨ë¯¸", "#ì ê·¹ìš´ìš©"]},
        {"id": 6, "name": "ë¯¸ë˜ì—ì…‹ì „ëµë°°ë¶„TDF2050", "type": "sell", "before": 33.00, "after": 32.63, "diff": "-0.37%", "region": "ê¸€ë¡œë²Œ", "category": "TDF", "tags": ["#ì€í‡´íƒ€ê²Ÿ2050", "#ìë™ìì‚°ë°°ë¶„", "#ê¸€ë¡œë²Œë¶„ì‚°"]}
    ]

    portfolio_profiles = {
        'ì„±ì¥í˜•': {
            "desc": "ì‹œì¥ ìˆ˜ìµë¥ ì„ ì´ˆê³¼í•˜ëŠ” ê³ ìˆ˜ìµì„ ì¶”êµ¬í•˜ë©°, ì£¼ì‹ ìì‚° ë¹„ì¤‘ì„ ê°€ì¥ ë†’ê²Œ ê°€ì ¸ê°‘ë‹ˆë‹¤.",
            "color": "#DC2626", # bg-red-600
            "riskLevel": 1,
            "items": [
                {"name": "í”¼ë¸ë¦¬í‹°ê¸€ë¡œë²Œí…Œí¬ë†€ë¡œì§€ì¦ê¶Œìíˆ¬ìì‹ íƒ", "category": "ì„ ì§„êµ­ì£¼ì‹", "ratio": 40.0, "tags": ["#ê¸€ë¡œë²Œê¸°ìˆ ì£¼", "#ì„±ì¥ì£¼", "#ITì„¹í„°"]},
                {"name": "í‚¤ì›€ìŠˆë¡œë”ì´ë¨¸ì§•ìœ„ë„ˆìŠ¤ì¦ê¶Œìíˆ¬ìì‹ íƒ", "category": "ì‹ í¥êµ­ì£¼ì‹", "ratio": 30.0, "tags": ["#ì‹ í¥êµ­", "#í•˜ì´ë¦¬ìŠ¤í¬", "#ê³ ì„±ì¥"]},
                {"name": "ë¯¸ë˜ì—ì…‹ì°¨ì´ë‚˜ê·¸ë¡œìŠ¤ì¦ê¶Œìíˆ¬ìì‹ íƒ", "category": "ì¤‘êµ­ì£¼ì‹", "ratio": 20.0, "tags": ["#ì¤‘êµ­ì„±ì¥ì£¼", "#ë³¸í† íˆ¬ì", "#ì†Œë¹„ì¬"]},
                {"name": "í•œí™”ì²œì—°ìì›ì¦ê¶Œìíˆ¬ìì‹ íƒ", "category": "ëŒ€ì²´íˆ¬ì", "ratio": 10.0, "tags": ["#ì›ìì¬", "#ë³€ë™ì„±", "#ì¸í”Œë ˆì´ì…˜"]},
            ]
        },
        'ì„±ì¥ì¶”êµ¬í˜•': {
            "desc": "ì ê·¹ì ì¸ ìì‚° ë°°ë¶„ì„ í†µí•´ ìì‚° ì¦ì‹ì„ ëª©í‘œë¡œ í•˜ë©°, ì£¼ì‹ ìœ„ì£¼ì— ì±„ê¶Œì„ ì¼ë¶€ í˜¼í•©í•©ë‹ˆë‹¤.",
            "color": "#EA580C", # bg-orange-600
            "riskLevel": 2,
            "items": [
                {"name": "ë¯¸ë˜ì—ì…‹ì „ëµë°°ë¶„TDF2050í˜¼í•©ìì‚°", "category": "TDF", "ratio": 35.0, "tags": ["#ì€í‡´íƒ€ê²Ÿ2050", "#ì£¼ì‹ë¹„ì¤‘í™•ëŒ€", "#ê¸€ë¡œë²Œë¶„ì‚°"]},
                {"name": "KCGIìƒëŸ¬ë¦¬ë§¨ì¦ê¶Œìíˆ¬ìì‹ íƒ", "category": "ê¸€ë¡œë²Œì£¼ì‹", "ratio": 25.0, "tags": ["#ê¸€ë¡œë²Œìš°ëŸ‰ì£¼", "#ESG", "#ì§€ë°°êµ¬ì¡°"]},
                {"name": "í‚¤ì›€ìŠˆë¡œë”ì´ë¨¸ì§•ìœ„ë„ˆìŠ¤ì¦ê¶Œìíˆ¬ìì‹ íƒ", "category": "ì‹ í¥êµ­ì£¼ì‹", "ratio": 15.0, "tags": ["#ì´ë¨¸ì§•ë§ˆì¼“", "#ê³ ì„±ì¥", "#ì ê·¹ìš´ìš©"]},
                {"name": "ì‚¼ì„±ë¯¸êµ­S&P500ì¸ë±ìŠ¤ì¦ê¶Œìíˆ¬ìì‹ íƒ", "category": "ì„ ì§„êµ­ì£¼ì‹", "ratio": 15.0, "tags": ["#ë¯¸êµ­ì§€ìˆ˜", "#ì„±ì¥ì£¼", "#ë‹¬ëŸ¬ìì‚°"]},
                {"name": "í•˜ë‚˜PIMCOê¸€ë¡œë²Œì¸ì»´í˜¼í•©ìì‚°", "category": "í•´ì™¸ì±„ê¶Œ", "ratio": 10.0, "tags": ["#ë°©ì–´ìì‚°", "#ì±„ê¶Œ", "#ì›”ì§€ê¸‰"]},
            ]
        },
        'ìœ„í—˜ì¤‘ë¦½í˜•': {
            "desc": "ìœ„í—˜ê³¼ ìˆ˜ìµì˜ ê· í˜•ì„ ì¤‘ì‹œí•˜ë©°, ì£¼ì‹ê³¼ ì±„ê¶Œì„ ê· í˜• ìˆê²Œ ë°°ë¶„í•©ë‹ˆë‹¤.",
            "color": "#2563EB", # bg-blue-600
            "riskLevel": 3,
            "items": [
                {"name": "ë¯¸ë˜ì—ì…‹ì „ëµë°°ë¶„TDF2035í˜¼í•©ìì‚°", "category": "TDF", "ratio": 40.0, "tags": ["#ìì‚°ë°°ë¶„", "#ê¸€ë¡œë²Œë¶„ì‚°", "#ì¤‘ìœ„í—˜"]},
                {"name": "ì‚¼ì„±ë¯¸êµ­S&P500ì¸ë±ìŠ¤ì¦ê¶Œìíˆ¬ìì‹ íƒ", "category": "ì„ ì§„êµ­ì£¼ì‹", "ratio": 20.0, "tags": ["#ë¯¸êµ­ëŒ€í‘œì§€ìˆ˜", "#ë‹¬ëŸ¬ìì‚°", "#ëŒ€í˜•ì£¼"]},
                {"name": "í•˜ë‚˜PIMCOê¸€ë¡œë²Œì¸ì»´í˜¼í•©ìì‚°", "category": "í•´ì™¸ì±„ê¶Œ", "ratio": 20.0, "tags": ["#ê¸€ë¡œë²Œì±„ê¶Œ", "#ì•ˆì •ì„±", "#ì¸ì»´ìˆ˜ìµ"]},
                {"name": "í•œí™”ì²œì—°ìì›ì¦ê¶Œìíˆ¬ìì‹ íƒ", "category": "ëŒ€ì²´íˆ¬ì", "ratio": 10.0, "tags": ["#ì›ìì¬", "#ì¸í”Œë ˆì´ì…˜í—·ì§€", "#ì‹¤ë¬¼ìì‚°"]},
                {"name": "Plusì‹ ì¢…ê°œì¸ìš©MMF", "category": "ìœ ë™ì„±", "ratio": 10.0, "tags": ["#ìœ ë™ì„±ê´€ë¦¬", "#ë‹¨ê¸°ìê¸ˆ", "#ìˆ˜ì‹œì…ì¶œê¸ˆ"]},
            ]
        },
        'ì•ˆì •ì¶”êµ¬í˜•': {
            "desc": "ì›ê¸ˆ ì†ì‹¤ ìœ„í—˜ì„ ë‚®ì¶”ë©´ì„œ ì‹œì¤‘ ê¸ˆë¦¬ +Î± ìˆ˜ìµì„ ì¶”êµ¬í•©ë‹ˆë‹¤.",
            "color": "#0D9488", # bg-teal-600
            "riskLevel": 4,
            "items": [
                {"name": "í•˜ë‚˜PIMCOê¸€ë¡œë²Œì¸ì»´í˜¼í•©ìì‚°", "category": "í•´ì™¸ì±„ê¶Œ", "ratio": 40.0, "tags": ["#ê¸€ë¡œë²Œì±„ê¶Œ", "#ì›”ì§€ê¸‰", "#ì•ˆì •ìˆ˜ìµ"]},
                {"name": "ë¯¸ë˜ì—ì…‹ì†”ë¡œëª¬ì¤‘ì¥ê¸°êµ­ê³µì±„", "category": "êµ­ë‚´ì±„ê¶Œ", "ratio": 30.0, "tags": ["#êµ­ê³µì±„", "#ì¤‘ê¸°íˆ¬ì", "#ì•ˆì „ìì‚°"]},
                {"name": "ë¯¸ë˜ì—ì…‹ì „ëµë°°ë¶„TDF2025í˜¼í•©ìì‚°", "category": "TDF", "ratio": 20.0, "tags": ["#ë³´ìˆ˜ì ë°°ë¶„", "#ì±„ê¶Œí˜¼í•©", "#ì€í‡´ì„ë°•"]},
                {"name": "êµë³´ì•…ì‚¬íŒŒì›Œì¸ë±ìŠ¤ì¦ê¶Œìíˆ¬ìì‹ íƒ", "category": "êµ­ë‚´ì£¼ì‹", "ratio": 10.0, "tags": ["#KOSPI200", "#ì¸ë±ìŠ¤", "#ì‹œì¥ìˆ˜ìµë¥ "]},
            ]
        },
        'ì•ˆì •í˜•': {
            "desc": "ì˜ˆê¸ˆ ìˆ˜ì¤€ì˜ ì•ˆì •ì„±ì„ ì¶”êµ¬í•˜ë©°, ë‹¨ê¸° ì±„ê¶Œ ë° ìœ ë™ì„± ìì‚° ìœ„ì£¼ë¡œ ìš´ìš©í•©ë‹ˆë‹¤.",
            "color": "#16A34A", # bg-green-600
            "riskLevel": 5,
            "items": [
                {"name": "Plusì‹ ì¢…ê°œì¸ìš©MMF", "category": "ìœ ë™ì„±", "ratio": 60.0, "tags": ["#ìˆ˜ì‹œì…ì¶œê¸ˆ", "#ì›ê¸ˆë³´ì¡´", "#ì´ˆë‹¨ê¸°"]},
                {"name": "ìš°ë¦¬ë‹¨ê¸°ì±„ê¶Œì¦ê¶Œìíˆ¬ìì‹ íƒ", "category": "êµ­ë‚´ì±„ê¶Œ", "ratio": 30.0, "tags": ["#êµ­ê³µì±„", "#ì•ˆì •ìˆ˜ìµ", "#ë‹¨ê¸°ì±„"]},
                {"name": "í•˜ë‚˜PIMCOê¸€ë¡œë²Œì¸ì»´í˜¼í•©ìì‚°", "category": "í•´ì™¸ì±„ê¶Œ", "ratio": 10.0, "tags": ["#ê¸€ë¡œë²Œì±„ê¶Œ", "#ì›”ì§€ê¸‰", "#ì±„ê¶Œí˜¼í•©"]},
            ]
        }
    }

    current_holdings = [
        {"id": 101, "name": "ë¯¸ë˜ì—ì…‹ì „ëµë°°ë¶„TDF2050", "ratio": 32.63, "amount": "40,920,000", "profit": "+15.2%"},
        {"id": 102, "name": "í•˜ë‚˜PIMCOê¸€ë¡œë²Œì¸ì»´í˜¼í•©ìì‚°", "ratio": 8.72, "amount": "10,930,000", "profit": "+3.1%"},
        {"id": 103, "name": "í‚¤ì›€ìŠˆë¡œë”ì´ë¨¸ì§•ìœ„ë„ˆìŠ¤", "ratio": 7.75, "amount": "9,720,000", "profit": "-1.5%"},
        {"id": 104, "name": "í•œí™”ì²œì—°ìì›ì¦ê¶Œìíˆ¬ìì‹ íƒ", "ratio": 4.74, "amount": "5,940,000", "profit": "0.0%"},
        {"id": 105, "name": "êµë³´ì•…ì‚¬íŒŒì›Œì¸ë±ìŠ¤", "ratio": 2.88, "amount": "3,610,000", "profit": "0.0%"},
        {"id": 106, "name": "Plusì‹ ì¢…ê°œì¸ìš©MMF", "ratio": 43.28, "amount": "54,290,000", "profit": "-"},
    ]
    
    # -----------------------------
    # 2. UI Layout (Mobile Frame)
    # -----------------------------
    # ì¤‘ì•™ ì •ë ¬ì„ ìœ„í•œ 3ë¶„í• , ê°€ìš´ë° ì»¬ëŸ¼(col_mobile)ë§Œ ì‚¬ìš©
    # Mobile Width Simulation
    _, col_mobile, _ = st.columns([1, 2, 1])
    
    with col_mobile:
        st.title("ğŸ¤– ë¡œë³´ ì–´ë“œë°”ì´ì €")
        
        tab1, tab2 = st.tabs(["ğŸ“Š ì˜¤ëŠ˜ì˜ í¬íŠ¸í´ë¦¬ì˜¤", "ğŸ’° ê³„ì¢Œ í˜„í™©"])

        # TAB 1: Portfolio View
        with tab1:
            col_header, col_hist = st.columns([4, 1])
            with col_header:
                st.caption("ê¸°ì¤€ì¼: 2026.01.14")
                st.subheader("AI ì¶”ì²œ í¬íŠ¸í´ë¦¬ì˜¤ âœ…")
            with col_hist:
                st.button("ğŸ“œ", key="history_btn", help="ì´ë ¥ ë³´ê¸°")

            # A. Investment Profile Selection
            profile_names = list(portfolio_profiles.keys())
            selected_profile = st.radio("íˆ¬ì ì„±í–¥ ì„ íƒ", profile_names, index=0, horizontal=True)
            
            profile = portfolio_profiles[selected_profile]
            is_my_profile = (selected_profile == 'ì„±ì¥í˜•')
            
            # HTML flattened with clean_html
            profile_html = clean_html(f"""
            <div style="background-color: {profile['color']}; padding: 24px; border-radius: 20px; color: white; margin-bottom: 24px; box-shadow: 0 10px 15px -3px rgba(0,0,0,0.1); position: relative; overflow: hidden;">
                <div style="position: absolute; right: -16px; top: -16px; background-color: rgba(255,255,255,0.1); width: 96px; height: 96px; border-radius: 50%; filter: blur(24px);"></div>
                <div style="position: relative; z-index: 10;">
                    <div style="display: flex; justify-content: space-between; align-items: start; margin-bottom: 8px;">
                        <div style="display: flex; align-items: center; gap: 4px;">
                            <span style="background-color: rgba(255,255,255,0.2); backdrop-filter: blur(4px); padding: 4px 8px; border-radius: 6px; font-size: 11px; font-weight: bold; display: inline-flex; align-items: center;">
                                ğŸ¯ ìœ„í—˜ë“±ê¸‰ {profile['riskLevel']}ë“±ê¸‰
                            </span>
                        </div>
                        {'<span style="background-color: white; color: #DC2626; padding: 4px 8px; border-radius: 999px; font-size: 10px; font-weight: bold; box-shadow: 0 1px 2px rgba(0,0,0,0.05);">ë‚˜ì˜ íˆ¬ìì„±í–¥</span>' if is_my_profile else ''}
                    </div>
                    <h3 style="margin: 0 0 4px 0; font-size: 20px; font-weight: 800; color:white;">{selected_profile} ì „ëµ</h3>
                    <p style="margin: 0; font-size: 13px; opacity: 0.9; color: rgba(255,255,255,0.9); line-height: 1.5;">{profile['desc']}</p>
                </div>
            </div>
            """)
            st.markdown(profile_html, unsafe_allow_html=True)

            # B. Items List
            st.markdown(f"**êµ¬ì„± ìƒí’ˆ ({len(profile['items'])}ê°œ)**")
            
            for item in profile['items']:
                # Card Style Container
                with st.container():
                    st.markdown(clean_html(f"""
                    <div class="card" style="display: flex; justify-content: space-between; align-items: center;">
                        <div style="flex: 1; padding-right: 16px;">
                            <div style="display: flex; gap: 4px; margin-bottom: 4px;">
                                <span style="font-size: 10px; background-color: #F3F4F6; color: #6B7280; padding: 2px 6px; border-radius: 4px; font-weight: 500;">{item['category']}</span>
                            </div>
                            <h4 style="margin: 0 0 8px 0; font-size: 14px; font-weight: bold; color: #111827; line-height: 1.3;">{item['name']}</h4>
                            <div style="display: flex; flex-wrap: wrap; gap: 4px;">
                                {''.join([f'<span style="font-size: 10px; color: #9CA3AF; background-color: #F9FAFB; padding: 2px 6px; border-radius: 4px;">{tag}</span>' for tag in item['tags']])}
                            </div>
                        </div>
                        <div style="width: 50px; height: 50px; background-color: #F9FAFB; border-radius: 12px; display: flex; flex-direction: column; justify-content: center; align-items: center; border: 1px solid #E5E7EB;">
                            <span style="font-size: 16px; font-weight: bold; color: #1F2937;">{item['ratio']}<span style="font-size: 10px;">%</span></span>
                            <span style="font-size: 9px; color: #9CA3AF;">ë¹„ì¤‘</span>
                        </div>
                    </div>
                    """), unsafe_allow_html=True)
            
            st.button(f"{selected_profile}ìœ¼ë¡œ ë³€ê²½ ì˜ˆì•½í•˜ê¸°", use_container_width=True, type="primary")
            st.caption("* ë³€ê²½ ì˜ˆì•½ ì‹œ ë‹¤ìŒ ë¦¬ë°¸ëŸ°ì‹± ì£¼ê¸°ì— ë°˜ì˜ë©ë‹ˆë‹¤.", help="ë§¤ì›” ë§ì¼ ê¸°ì¤€")

        # TAB 2: Status View
        with tab2:
            is_rebalancing = st.toggle("ë¦¬ë°¸ëŸ°ì‹± ì§„í–‰ì¤‘ (Demo)", value=True)
            
            # Account Header
            st.markdown(clean_html("""
            <div style="display: flex; justify-content: space-between; align-items: start; margin-bottom: 16px;">
                <div>
                    <div style="display: flex; align-items: center; gap: 6px; margin-bottom: 4px;">
                        <span style="background-color: #F3F4F6; color: #4B5563; padding: 2px 6px; border-radius: 4px; font-size: 10px; font-weight: bold;">ì—°ê¸ˆì €ì¶•</span>
                        <span style="font-size: 13px; font-weight: bold; color: #1F2937; text-decoration: underline; text-decoration-color: #D1D5DB; text-underline-offset: 4px;">123-45-678910</span>
                    </div>
                    <h2 style="margin: 0; font-size: 18px; font-weight: 800; color: #111827;">Global Quants EMP</h2>
                </div>
            </div>
            """), unsafe_allow_html=True)
            
            # Dark Card (Performance) with Gradient
            st.markdown(clean_html("""
            <div style="background: linear-gradient(135deg, #111827, #1F2937); color: white; padding: 20px; border-radius: 20px; margin-bottom: 24px; box-shadow: 0 10px 15px -3px rgba(0,0,0,0.2); position: relative; overflow: hidden;">
                <div style="position: absolute; right: -20px; top: -20px; background-color: rgba(255,255,255,0.05); width: 128px; height: 128px; border-radius: 50%; filter: blur(30px);"></div>
                <div style="position: relative; z-index: 10;">
                    <div style="display: flex; justify-content: space-between; margin-bottom: 16px;">
                        <div>
                            <div style="color: #9CA3AF; font-size: 12px; margin-bottom: 4px; font-weight: 500;">ì´ í‰ê°€ê¸ˆì•¡</div>
                            <div style="font-size: 24px; font-weight: 800; letter-spacing: -0.5px;">125,430,000ì›</div>
                        </div>
                        <div style="text-align: right;">
                            <div style="color: #9CA3AF; font-size: 12px; margin-bottom: 4px; font-weight: 500;">ëˆ„ì  ìˆ˜ìµë¥ </div>
                            <div style="font-size: 20px; font-weight: 800; color: #F87171;">+12.4%</div>
                        </div>
                    </div>
                    <div style="border-top: 1px solid rgba(255,255,255,0.1); padding-top: 16px; display: flex; gap: 12px;">
                        <div style="flex: 1;">
                            <div style="display: flex; justify-content: space-between; font-size: 11px; margin-bottom: 6px;">
                                <span style="color: #9CA3AF;">ë²¤ì¹˜ë§ˆí¬ (KOSPI)</span>
                                <span style="color: #E5E7EB; font-weight: bold;">+9.2%</span>
                            </div>
                            <div style="background-color: #374151; height: 6px; border-radius: 999px; overflow: hidden;">
                                <div style="background-color: #9CA3AF; width: 70%; height: 100%;"></div>
                            </div>
                        </div>
                        <div style="width: 1px; background-color: rgba(255,255,255,0.1);"></div>
                        <div style="flex: 1;">
                            <div style="display: flex; justify-content: space-between; font-size: 11px; margin-bottom: 6px;">
                                <span style="color: #9CA3AF;">ë‚´ í¬íŠ¸í´ë¦¬ì˜¤</span>
                                <span style="color: #F87171; font-weight: bold;">+12.4%</span>
                            </div>
                            <div style="background-color: #374151; height: 6px; border-radius: 999px; overflow: hidden;">
                                <div style="background-color: #EF4444; width: 90%; height: 100%;"></div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            """), unsafe_allow_html=True)
            
            if is_rebalancing:
                # Rebalancing Status Card
                st.markdown(clean_html("""
                <div style="background-color: #2563EB; border-radius: 16px; padding: 16px; color: white; margin-bottom: 20px; box-shadow: 0 4px 6px -1px rgba(37, 99, 235, 0.2);">
                    <div style="display: flex; items-center; gap: 8px; margin-bottom: 8px;">
                        <!-- Spinner Icon simulation -->
                        <span style="font-size: 16px;">ğŸ”„</span>
                        <span style="font-weight: bold; font-size: 15px;">ë¦¬ë°¸ëŸ°ì‹± ì§„í–‰ ì¤‘</span>
                    </div>
                    <p style="font-size: 12px; color: #DBEAFE; margin: 0 0 12px 0; line-height: 1.4;">
                        ì‹œì¥ ìƒí™© ë³€í™”ì— ë§ì¶° ìì‚° ë¹„ì¤‘ì„ 'ì„±ì¥í˜•' ëª¨ë¸ë¡œ ì¡°ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤.
                    </p>
                    <div style="background-color: #1E40AF; height: 6px; border-radius: 999px; overflow: hidden; margin-bottom: 6px;">
                        <div style="background-color: white; width: 65%; height: 100%;"></div>
                    </div>
                    <div style="display: flex; justify-content: space-between; font-size: 10px; color: #BFDBFE;">
                        <span>ë§¤ë„ ì™„ë£Œ</span>
                        <span>ë§¤ìˆ˜ ì¤‘ (65%)</span>
                    </div>
                </div>
                """), unsafe_allow_html=True)

                st.markdown(clean_html("""
                <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 12px;">
                    <h3 style="margin: 0; font-size: 16px; font-weight: bold; color: #111827;">ì‹¤ì‹œê°„ ë³€ê²½ í˜„í™©</h3>
                    <span style="background-color: #F3F4F6; color: #6B7280; font-size: 10px; padding: 2px 8px; border-radius: 999px; font-weight: bold;">Live</span>
                </div>
                """), unsafe_allow_html=True)
                
                # Group by category logic
                cats = {}
                for item in ongoing_changes:
                    cat = item['category']
                    if cat not in cats: cats[cat] = []
                    cats[cat].append(item)
                
                sorted_cats = sorted(cats.keys(), key=lambda k: len(cats[k]), reverse=True)
                
                for cat in sorted_cats:
                    # Category Header
                    st.markdown(clean_html(f"""
                    <div style="background-color: #F9FAFB; padding: 10px 16px; border-top-left-radius: 12px; border-top-right-radius: 12px; border: 1px solid #F3F4F6; border-bottom: none; margin-top: 12px;">
                        <span style="font-size: 12px; font-weight: bold; color: #374151;">{cat}</span>
                    </div>
                    """), unsafe_allow_html=True)
                    
                    # Items
                    for idx, item in enumerate(cats[cat]):
                        is_last = (idx == len(cats[cat]) - 1)
                        border_style = "border-bottom-left-radius: 12px; border-bottom-right-radius: 12px;" if is_last else ""
                        
                        # Type Badge Colors
                        type_colors = {
                            "new": ("#FEF2F2", "#DC2626", "ì‹ ê·œí¸ì…"), # bg-red-50, text-red-600
                            "out": ("#EFF6FF", "#2563EB", "ì „ëŸ‰ë§¤ë„"), # bg-blue-50, text-blue-600
                            "buy": ("#FEF2F2", "#DC2626", "ë¹„ì¤‘í™•ëŒ€"),
                            "sell": ("#EFF6FF", "#2563EB", "ë¹„ì¤‘ì¶•ì†Œ")
                        }
                        bg_c, text_c, label = type_colors.get(item['type'], ("#F3F4F6", "#6B7280", item['type']))
                        
                        diff_color = "#DC2626" if item['diff'].startswith('+') else "#2563EB"
                        
                        st.markdown(clean_html(f"""
                        <div style="background-color: white; padding: 16px; border: 1px solid #F3F4F6; {border_style} touch-action: manipulation;">
                            <div style="display: flex; justify-content: space-between; margin-bottom: 12px;">
                                <div>
                                    <div style="margin-bottom: 4px;">
                                        <span style="background-color: {bg_c}; color: {text_c}; padding: 2px 6px; border-radius: 4px; font-size: 10px; font-weight: bold;">{label}</span>
                                    </div>
                                    <h4 style="margin: 0; font-size: 14px; font-weight: bold; color: #1F2937; margin-bottom: 4px;">{item['name']}</h4>
                                    <div style="display: flex; gap: 4px;">
                                        {''.join([f'<span style="font-size: 10px; color: #9CA3AF; border: 1px solid #F3F4F6; padding: 2px 6px; border-radius: 4px;">{tag}</span>' for tag in item['tags']])}
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Visualization Before -> After -->
                            <div style="display: flex; align-items: center; gap: 12px; background-color: #F9FAFB; padding: 10px; border-radius: 8px;">
                                <div style="text-align: center; width: 40px;">
                                    <div style="font-size: 9px; color: #9CA3AF; margin-bottom: 2px;">ì´ì „</div>
                                    <div style="font-size: 11px; font-weight: 500; color: #6B7280;">{item['before']}%</div>
                                </div>
                                <div style="flex: 1; display: flex; align-items: center; gap: 4px;">
                                    <div style="flex: 1; height: 1px; background-color: {bg_c};"></div>
                                    <div style="background-color: {bg_c}; color: {text_c}; border: 1px solid {bg_c}; padding: 2px 8px; border-radius: 999px; font-size: 10px; font-weight: bold;">
                                        {item['diff']}
                                    </div>
                                    <div style="flex: 1; height: 1px; background-color: {bg_c};"></div>
                                </div>
                                <div style="text-align: center; width: 40px;">
                                    <div style="font-size: 9px; color: #9CA3AF; margin-bottom: 2px;">í˜„ì¬</div>
                                    <div style="font-size: 11px; font-weight: bold; color: {text_c};">{item['after']}%</div>
                                </div>
                            </div>
                        </div>
                        """), unsafe_allow_html=True)
            
            else:
                st.subheader("í˜„ì¬ ë³´ìœ  ìì‚°")
                for holding in current_holdings:
                    profit_color = "#DC2626" if holding['profit'].startswith('+') else ("#2563EB" if holding['profit'].startswith('-') else "#6B7280")
                    st.markdown(clean_html(f"""
                    <div class="card" style="display: flex; justify-content: space-between; align-items: center;">
                        <div>
                            <h4 style="margin: 0 0 4px 0; font-size: 14px; font-weight: bold; color: #1F2937;">{holding['name']}</h4>
                            <div style="display: flex; align-items: center; gap: 6px;">
                                <span style="font-size: 12px; color: #6B7280;">{holding['amount']}ì›</span>
                                <span style="font-size: 10px; background-color: #F3F4F6; color: #6B7280; padding: 2px 6px; border-radius: 4px;">{holding['ratio']}%</span>
                            </div>
                        </div>
                        <div style="text-align: right;">
                            <span style="font-size: 14px; font-weight: bold; color: {profit_color};">{holding['profit']}</span>
                        </div>
                    </div>
                    """), unsafe_allow_html=True)

if selection == "ğŸ¤– ë¡œë³´ ì–´ë“œë°”ì´ì € (Demo)":
    page_robo_advisor()

if selection == "ğŸ§ª Qlib ì‹¤í—˜ì‹¤ (Pro)":
    st.title("ğŸ§ª Qlib ì‹¤í—˜ì‹¤ (Pro)")
    st.caption("Microsoft Qlib ìŠ¤íƒ€ì¼ì˜ ì „ë¬¸ì ì¸ í€€íŠ¸ ì—°êµ¬ ì›Œí¬í”Œë¡œìš°ë¥¼ ì œê³µí•©ë‹ˆë‹¤. Factor IC ë¶„ì„ì„ í†µí•´ ì•ŒíŒŒë¥¼ ê²€ì¦í•˜ì„¸ìš”.")
    
    # 1. ì‚¬ì´ë“œë°” ì„¤ì • (Dataset & Model)
    with st.sidebar:
        st.header("ğŸ”¬ ì‹¤í—˜ ì„¤ì • (Experiment Config)")
        
        # Universe Reuse
        universe_preset = st.selectbox(
            "ìœ ë‹ˆë²„ìŠ¤ ì„ íƒ", 
            ["NASDAQ Top 10 (Demo)", "Tech Giants (M7)", "NASDAQ Top 30", "ì§ì ‘ ì…ë ¥"]
        )
        
        if universe_preset == "ì§ì ‘ ì…ë ¥":
            tickers_input = st.text_input("ì¢…ëª© ì½”ë“œ (ì‰¼í‘œ êµ¬ë¶„)", "AAPL, MSFT, GOOGL")
            tickers = [t.strip().upper() for t in tickers_input.split(",") if t.strip()]
        elif universe_preset == "Tech Giants (M7)":
            tickers = ["AAPL", "MSFT", "GOOGL", "AMZN", "NVDA", "META", "TSLA"]
        elif universe_preset == "NASDAQ Top 10 (Demo)":
            tickers = ["AAPL", "MSFT", "NVDA", "GOOGL", "AMZN", "META", "TSLA", "AVGO", "COST", "PEP"]
        else: # NASDAQ Top 30
            # Sample subset
            tickers = ["AAPL", "MSFT", "NVDA", "GOOGL", "AMZN", "META", "TSLA", "AVGO", "COST", "PEP", "CSCO", "NFLX", "AMD"]

        st.divider()
        start_date = st.date_input("ë°ì´í„° ì‹œì‘ì¼", pd.to_datetime("2020-01-01"))
        split_date = st.date_input("í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• ì¼", pd.to_datetime("2023-01-01"))
        
        st.divider()
        st.subheader("âš™ï¸ ëª¨ë¸ & íŒ©í„° ì„¤ì • (Pro)")
        
        # Factor Groups (Mock for now, will filter later if needed)
        factor_groups = st.multiselect(
            "í™œì„±í™”í•  íŒ©í„° ê·¸ë£¹",
            ["Price Momentum (ROC, KMID)", "Volatility (Std, ATR)", "Volume (VMA)"],
            default=["Price Momentum (ROC, KMID)", "Volatility (Std, ATR)", "Volume (VMA)"]
        )
        
        st.caption("LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„°")
        col_p1, col_p2 = st.columns(2)
        with col_p1:
            lgbm_leaves = st.slider("Num Leaves", 10, 255, 31)
            lgbm_depth = st.slider("Max Depth", -1, 20, -1)
        with col_p2:
            lgbm_lr = st.select_slider("Learning Rate", options=[0.001, 0.005, 0.01, 0.05, 0.1], value=0.05)
            lgbm_min_data = st.slider("Min Data Leaf", 10, 100, 20)
            
        with st.expander("ê³ ê¸‰ ì„¤ì • (Advanced)"):
            lgbm_feature_frac = st.slider("Feature Fraction", 0.5, 1.0, 0.8, help="íŠ¸ë¦¬ ìƒì„± ì‹œ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•  Feature ë¹„ìœ¨")
            lgbm_bagging_frac = st.slider("Bagging Fraction", 0.5, 1.0, 0.8, help="ë°ì´í„° ìƒ˜í”Œë§ ë¹„ìœ¨")
        
    # 2. Main Workspace
    st.info("ğŸ’¡ **Qlib Workflow**: Data Loader -> Alpha Factory (Feature Eng.) -> Label Gen -> LightGBM -> IC Analysis")
    
    if st.button("ğŸš€ ì‹¤í—˜ ì‹œì‘ (Run Experiment)"):
        import qlib_workflow
        import importlib
        importlib.reload(qlib_workflow) # Reload for dev
        from qlib_workflow import QlibWorkflow
        
        # 2.1 Data Loading (Inline OR Lambda)
        def my_loader(tickers, start, end):
            # Wrapper around yf.download
            # Note: We need a buffer for features (e.g. 60 days)
            actual_start = pd.to_datetime(start) - pd.Timedelta(days=100)
            with st.spinner(f"ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘... ({len(tickers)} ì¢…ëª©)"):
                try:
                    df = yf.download(tickers, start=actual_start, end=end, group_by='ticker', progress=False)
                    # Convert MultiIndex Columns to Dict of DFs
                    res = {}
                    if len(tickers) == 1:
                        res[tickers[0]] = df
                    else:
                        for t in tickers:
                            try:
                                res[t] = df[t].dropna()
                            except: pass
                    return res
                except Exception as e:
                    st.error(f"Download Error: {e}")
                    return {}

        qc = QlibWorkflow(my_loader)
        
        # 2.2 Orchestration
        with st.status("ğŸ”¬ í€€íŠ¸ íŒŒì´í”„ë¼ì¸ ê°€ë™ ì¤‘...", expanded=True) as status:
            st.write("1ï¸âƒ£ ë°ì´í„° ì¤€ë¹„ ë° ì•ŒíŒŒ ìƒì„± ì¤‘...")
            dataset, err = qc.prepare_data(tickers, start_date, pd.to_datetime("today"))
            
            if err or dataset is None:
                st.error(f"ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {err}")
                st.stop()
                
            st.success(f"ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ! (Shape: {dataset.shape})")
            st.write(f"ğŸ“Š ìƒì„±ëœ íŒ©í„°(Features): {len(dataset.columns)-1}ê°œ")
            st.dataframe(dataset.head(5))
            
            # Split
            st.write("2ï¸âƒ£ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  ì¤‘...")
            dates = dataset.index.get_level_values('Date')
            train_mask = dates < pd.to_datetime(split_date)
            test_mask = dates >= pd.to_datetime(split_date)
            
            train_df = dataset[train_mask]
            test_df = dataset[test_mask]
            
            # Feature Cols (All except Ref($close, -1))
            label_col = [c for c in dataset.columns if "Ref" in c][0]
            feature_cols = [c for c in dataset.columns if c != label_col]
            
            st.write(f"Train: {len(train_df)} rows, Test: {len(test_df)} rows")
            
            st.write("3ï¸âƒ£ LightGBM ëª¨ë¸ í•™ìŠµ ì¤‘...")
            
            # Pass Params
            lgbm_params = {
                'num_leaves': lgbm_leaves,
                'learning_rate': lgbm_lr,
                'max_depth': lgbm_depth,
                'min_child_samples': lgbm_min_data,
                'colsample_bytree': lgbm_feature_frac,
                'subsample': lgbm_bagging_frac,
                'n_estimators': 300
            }
            
            model = qc.train_model(train_df, test_df, feature_cols, label_col, **lgbm_params) 
            st.success("ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
            
            st.write("4ï¸âƒ£ ì„±ê³¼ ë¶„ì„ (IC Analysis) ì¤‘...")
            metrics, daily_ic, daily_rank_ic, res_df = qc.analyze_performance(model, test_df, feature_cols, label_col)
            
            status.update(label="âœ… ì‹¤í—˜ ì™„ë£Œ!", state="complete", expanded=False)
            
        # 3. Report
        st.divider()
        st.header("ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ë¦¬í¬íŠ¸ (Experiment Report)")
        
        # Metrics Cards
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("IC (Mean)", f"{metrics['IC_Mean']:.4f}")
        c2.metric("ICIR", f"{metrics['ICIR']:.4f}")
        c3.metric("Rank IC (Mean)", f"{metrics['Rank_IC_Mean']:.4f}")
        c4.metric("Rank ICIR", f"{metrics['Rank_ICIR']:.4f}")
        
        st.caption("""
        * **IC (Information Coefficient)**: ì˜ˆì¸¡ê³¼ ì‹¤ì œ ìˆ˜ìµë¥ ì˜ ìƒê´€ê³„ìˆ˜. (0.05 ì´ìƒì„ í›Œë¥­)
        * **ICIR**: ICì˜ ì•ˆì •ì„± (Mean / Std). ë†’ì„ìˆ˜ë¡ ê¾¸ì¤€í•œ ì˜ˆì¸¡ë ¥.
        """)
        
        # Charts
        tab1, tab2, tab3 = st.tabs(["ğŸ“‰ ëˆ„ì  IC (Cumulative IC)", "ğŸ” Feature Importance", "ğŸ“ˆ ì˜ˆì¸¡ vs ì‹¤ì œ"])
        
        with tab1:
            st.subheader("ì¼ë³„ Rank IC ì¶”ì´")
            cum_rank_ic = daily_rank_ic.cumsum()
            st.line_chart(cum_rank_ic)
            st.caption("ìš°ìƒí–¥í• ìˆ˜ë¡ ëª¨ë¸ì´ ê¾¸ì¤€íˆ ì‹œì¥ì„ ë§ì¶”ê³  ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.")
            
        with tab2:
            st.subheader("ì£¼ìš” íŒ©í„° ì¤‘ìš”ë„ (Top 10)")
            imp = pd.DataFrame({
                "Feature": feature_cols,
                "Importance": model.feature_importances_
            }).sort_values(by="Importance", ascending=False).head(10)
            
            import plotly.express as px
            fig = px.bar(imp, x='Importance', y='Feature', orientation='h', title="Feature Importance")
            st.plotly_chart(fig)
            
        with tab3:
            st.subheader("í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì˜ˆì¸¡ ë¶„í¬")
            st.scatter_chart(res_df.reset_index(), x='Pred', y=label_col, color='#00CC96')
